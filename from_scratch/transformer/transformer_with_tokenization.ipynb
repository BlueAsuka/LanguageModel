{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download() # if not install the nltk library then uncomment this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'love', 'this', 'flavor', '!', 'It', \"'s\", 'by', 'far', 'the', 'best', 'choice', 'and', 'my', 'go-to', 'whenever', 'I', 'go', 'to', 'the', 'grocery', 'store', '.', 'I', 'wish', 'they', 'would', 'restock', 'it', 'more', 'often', 'though', '.']\n"
     ]
    }
   ],
   "source": [
    "text = 'I love this flavor! It\\'s by far the best choice and my go-to whenever I go to the grocery store. I wish they would restock it more often though.'\n",
    "\n",
    "word_tokens = word_tokenize(text)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14310"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = sorted(list(set(word_tokenize(text))))\n",
    "vocab_size = len(words)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { w:i for i,w in enumerate(words) }\n",
    "itos = { i:w for i,w in enumerate(words) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ' '.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3053, 3512, 3324, 11053, 10791, 13010, 5723, 12819, 13010, 6533, 225]\n",
      "You are all resolved rather to die than to famish ?\n"
     ]
    }
   ],
   "source": [
    "test_string = 'You are all resolved rather to die than to famish?'\n",
    "print(encode(word_tokenize(test_string)))\n",
    "print(decode(encode(word_tokenize(test_string))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([254509]) <built-in method type of Tensor object at 0x00000282E217D4E0>\n",
      "tensor([ 1152,   709,   223,   482, 13877, 10480,  3440,  7080,   219,  7604,\n",
      "         8993, 12087,   221,   323,   223,  2520,   219, 12087,   221,  1152,\n",
      "          709,   223,  3053,  3512,  3324, 11053, 10791, 13010,  5723, 12819,\n",
      "        13010,  6533,   225,   323,   223,  2256,   221, 11053,   221,  1152,\n",
      "          709,   223,  1152,   219, 14291,  8402,   640,  1769,  8232,  4679,\n",
      "         6251, 13010, 12831, 10036,   221,   323,   223,  2919,  8404,   219,\n",
      "        13877,  8404,   221,  1152,   709,   223,  1679, 13581,  8340,  7738,\n",
      "          219,  3412, 13877,   162,  7567,  5147,  3596,  9761,  9833, 10439,\n",
      "          221,  1547,  3061, 13661,   225,   323,   223,  1924,  9271, 12703,\n",
      "         9583,  9377,   224,  8580,  8243,  3786,  5952,   223,  3659,   219,\n",
      "         3659,     0,  2370,   709,   223,  1972, 14183,   219,  7277,  4738,\n",
      "          221,  1152,   709,   223,  2919,  3512,  3145, 10283,  4738,   219,\n",
      "        12831,  9969,  7277,   221,  2943,  3643, 12582,  9697, 14214, 10931,\n",
      "        13581,   223,  7937, 12857, 14214, 14282, 13581,  4389, 12831, 12548,\n",
      "          219, 14003,  8243, 13966, 14029,   219, 13877,  9103,  7425, 12857,\n",
      "        10932, 13581,  7886,   224,  4389, 12857, 12873, 13877,  3512, 13038,\n",
      "         5479,   223, 12831,  8529, 12828,  3257, 13581,   219, 12831,  9629,\n",
      "         9659,  9761,  9181,   219,  8232,  3552,  3400,  8211, 13010,  9928,\n",
      "        12835,  3115,   224,  9761, 12505,  8232,  3061,  7090, 13010, 12837,\n",
      "         1679, 13581, 11097, 12883, 14121,  9761, 10136,   219,  6329, 13877,\n",
      "         3827, 10753,   223,  6877, 12831,  7269,  8402,  1478, 12087, 12883,\n",
      "         8017,  7898,  6877,  4245,   219,  9545,  8017, 12880,  6877, 11097,\n",
      "          221,  2370,   709,   223,  3027, 14291, 10480,  6338,  3277,   640,\n",
      "         1769,   225,   323,   223,   301,  7738,  6732,   223,  7585,   191,\n",
      "         3061, 13671,  5936, 13010, 12831,  4901,   221,  2370,   709,   223,\n",
      "          778, 14291, 13973, 11572,  7585,  7541,  5952,  6877,  7748,  5203,\n",
      "          225,  1152,   709,   223,  2870, 13943,   224,  3412,  5176,  3786,\n",
      "         5078, 13010,  7211,  7738,  7277, 10999,  6946,   219,  4389, 12828,\n",
      "         7585,  9984,  7739, 14121,  3906, 10582,   221,  2370,   709,   223,\n",
      "         1895,   219,  4389, 12087,  9545,  8879,   221,  1152,   709,   223,\n",
      "         1478, 11365, 13533, 14291,   219, 13973,  7585,  7559,  5952,  6535,\n",
      "          219,  7585,  5721,  8243, 13010, 12828,  6236,   223, 12894, 11986,\n",
      "         9047,  4455,  3786,  5078, 13010, 11365,  8243, 13840,  6877,  7748,\n",
      "         5203,  7585,  5721,  8243, 13010, 10224,  7748,  9284,  3412, 13010,\n",
      "         3786,  9934, 10582,   224, 14002,  7585,  8232,   219,  6359, 12982,\n",
      "        12831,  3361,  9659,  7748, 13727,   221,  2370,   709,   223,  2943,\n",
      "         7585,  4455,  9545,  7673,  8017,  7748,  9400,   219, 14291,  3143,\n",
      "         3061, 13684,  8017,  7738,   221,  3053,  9356,  8017,  9510, 13874,\n",
      "        11365,  7585,  8232,  5243,   221,  1152,   709,   223,  1488,  1478,\n",
      "         9356,  9545,   219,  1478,  9426,  9545,  3786,  3745,  9659,  3150,\n",
      "          224,  7585,  7559,  6583,   219, 14121, 12592,   219, 13010, 13000,\n",
      "         8017, 10990,   221,  2943, 11721,  3512, 12856,   225,  2661,  9756,\n",
      "        11770,  9583,     3, 12831,  4739,  8232, 11168,   223, 14042, 12263,\n",
      "        13877, 10353,  7694,   225, 13010, 12831,   659,     0,   323,   223,\n",
      "          735,   219,  4862,   221,  1152,   709,   223,  2488,     0, 14025,\n",
      "         4866,  7694,   225,  2370,   709,   223,  3024,  1815,   307,   224,\n",
      "         9701, 12828,  7559,  3364,  8775, 12831, 10036,   221,  1152,   709,\n",
      "          223,  1376,   191,  9701,  7793,  6277,   223, 14214,  3324, 12831,\n",
      "        11064, 13966, 11970,     0,  1740,   223,  2943, 14186,   191,   219,\n",
      "         9372,  5205,   219,  8017,  7481,   225, 13984,  7253, 14291,  2999,\n",
      "         3769,  3412,  4802,   225,  2661,  8981,   225, 12087,   219,  1478,\n",
      "        10356, 14291,   221,  1152,   709,   223,  1981,  4384,  8232,  9545,\n",
      "        13422, 13010, 12831, 11530,   224, 12857,  7567,  7453,  8122, 12883,\n",
      "         6953, 13973, 13877,  8174, 13010,  5928,   219, 14002,  9564, 13877,\n",
      "          162, 11723,   134,  8017,  5528,   221,  2673, 11365, 10283, 12520,\n",
      "         7567, 12428,  4264,   223, 12857, 11615,  8402, 13877,  7567, 12428,\n",
      "         3530, 13038,   221,  1740,   223,  2981,   219,  8969,   219,  9372,\n",
      "         7277,  7026,   219,  9132,  7793,  9446,   219,  2984, 14291, 13358,\n",
      "        14303,   225,  1152,   709,   223,  2919,  4455,  9545,   219, 11835,\n",
      "          219, 13877,  3512, 13360,  3353,   221,  1740,   223,  1478, 12761,\n",
      "        14291,   219,  7026,   219,  9283,  4625,  4496,  1374, 12831,  9969,\n",
      "         9659, 14291,   221,  1180, 14300, 13813,   219,  3058, 12507,  8017,\n",
      "        12883,  5487,   219, 14291,  8986,  3552, 13943,  2568,  3596, 12831,\n",
      "         7633, 14121, 14300, 12262,  3552,  8624, 12837,   301, 12831,  2291,\n",
      "        12254,   219, 14038,  5212, 14065,  9697,  2661, 13874,  8243, 12693,\n",
      "          219,  5263, 12785, 12897,  5360,  1958,  9271, 12428,  8669,  3595,\n",
      "        12819,  4455,  6363,   373,  8017, 14300,  7982,   221,  1180, 12831,\n",
      "         5487,   219,  2661,  7269,   219,  9545, 12831,  9969,   219,  8863,\n",
      "         8243,   219,  3412,  3058,  8387, 13010, 12837,   219,  9545,  3530,\n",
      "          219,  9356,  7673,   221,   314,   219,  3053,  3512, 13114,  4412,\n",
      "         4429,  2684, 13984,  9271,  3615, 14291,   219,  3412, 14291, 11872,\n",
      "         2661,  7672,  9583,     3, 12831, 12254,   219, 14025,  4496,  6877,\n",
      "        14291,  8636,  6576,   219,  2946, 14291,  5376, 12837,  3552,  6250,\n",
      "          221,  1152,   709,   223,   664,  6877, 13581,     0,  2763,   219,\n",
      "         8049,     0,  2673,  9408,  4499,  6877, 13581, 14278,   223, 12504,\n",
      "        13581, 13010,  6533,   219,  3412, 12835, 12353,  5274, 14121,  7315,\n",
      "          224,  8863,  6158,  6877, 13600,   219, 13010, 12561, 13591,   224,\n",
      "        10984,  5404,  3440, 14029,  3173,  6346,  3277, 12831, 11129,   219,\n",
      "         3412, 10596,  9271, 10133, 12261,  5404,   219, 13010,  4587, 13559,\n",
      "         3412, 11074, 12831, 10283,   221,  1488, 12831, 13838,  6146, 13581,\n",
      "         9545, 13559,   219, 12857, 14065,   224,  3412, 12843,   191,  3324,\n",
      "        12831,  8770, 12857,  3796, 13581,   221,  1740,   223,  1020, 14291,\n",
      "         9356,   768, 14303, 14168,  8878,   219,  1979,  3786,  3152,  9659,\n",
      "         6856,   221,  1478, 11615, 12761, 14291,   226, 10429, 12697,   223,\n",
      "         8243,  8986,  3786, 14291,  7567,  7607,  8243,   224,   618,   219,\n",
      "        11807,  8243, 11569,  9372, 10654,   219,  1478, 14065, 13658,  2722,\n",
      "        12221,   202,  3061,  8684,  9271,   221,  1152,   709,   223,  2930,\n",
      "          219,  1478,   162,  7604,  8243,   219, 11835,   223, 14278, 14291,\n",
      "         9356,  9545, 12873, 13010,  6834,  9660,  9761,  5813, 14121,  3061,\n",
      "        12697,   223,  4389,   219,  3400,   202, 10224, 14291,   219,  5582,\n",
      "          221,  1740,   223,  2665, 13840,  3061, 12986, 13981,  3324, 12831,\n",
      "         4132,   191,  9045,  2214,   130,  3277, 12831,  3916,   219, 12963,\n",
      "         3152,  8243,   223,  2660,  9704,  8636,  3061,  7439,  8243,  5721,\n",
      "        10938,  1478,     3, 12831,  9101,  9583,     3, 12831,  4132,   219,\n",
      "         7932,  3412, 13293,   219,  2556,  5356, 12831, 13681,   219,  9460,\n",
      "         3804,  1693,  8414, 14121, 12831, 11064,   219, 13984, 12831,  9756,\n",
      "         8161,   919, 11488,  3412,  7604,   219,  5698,   219,  8155,   219,\n",
      "        13793,   219,  6626,   219,   350,   219,  9370,  9926,   219,  5721,\n",
      "         9139,  2833, 12831,  3473,  3412,  3249,  4899,  1958, 12831, 14028,\n",
      "         4132,   221,  2661,  3916,  3430,   130,   220,  1152,   709,   223])\n"
     ]
    }
   ],
   "source": [
    "# encode the entire text dataset and store it into a torch.Tensor\n",
    "data = torch.tensor(encode(word_tokenize(text)), dtype=torch.long)\n",
    "print(data.shape, data.type)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now split up the data into train and validation sets\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1152,   709,   223,   482, 13877, 10480,  3440,  7080,   219])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: tensor([1152]), target: 709\n",
      "input: tensor([1152,  709]), target: 223\n",
      "input: tensor([1152,  709,  223]), target: 482\n",
      "input: tensor([1152,  709,  223,  482]), target: 13877\n",
      "input: tensor([ 1152,   709,   223,   482, 13877]), target: 10480\n",
      "input: tensor([ 1152,   709,   223,   482, 13877, 10480]), target: 3440\n",
      "input: tensor([ 1152,   709,   223,   482, 13877, 10480,  3440]), target: 7080\n",
      "input: tensor([ 1152,   709,   223,   482, 13877, 10480,  3440,  7080]), target: 219\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f'input: {context}, target: {target}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # How many independent sequences will be process in parallel?\n",
    "block_size = 8 # What is the maximum context length for predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x.to(DEVICE), y.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[ 1478,  8532,  9659, 14291,   219,   392,  9701, 12828],\n",
      "        [ 1478,   162,  9460,  9975,  3276,   219,  9460, 12228],\n",
      "        [  223,   391, 12891,  3061,  8889,   225, 12968,  6928],\n",
      "        [ 8348,  8017,  7560,   223,  2943,  4872,  7857,  4468]],\n",
      "       device='cuda:0')\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[ 8532,  9659, 14291,   219,   392,  9701, 12828,  3512],\n",
      "        [  162,  9460,  9975,  3276,   219,  9460, 12228, 12306],\n",
      "        [  391, 12891,  3061,  8889,   225, 12968,  6928,  5304],\n",
      "        [ 8017,  7560,   223,  2943,  4872,  7857,  4468, 12891]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for b in range(batch_size): # batch dimension\n",
    "#     print(f'batch {b+1}/{batch_size}')\n",
    "#     for t in range(block_size): # time dimension\n",
    "#         context = xb[b, :t+1]\n",
    "#         target = yb[b,t]\n",
    "#         print(f\"when input is {context.tolist()} the target: {target}\")\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 32 # how many independent sequences will we process in parallel?\n",
    "block_size = 128 # what is the maximum context length for predictions?\n",
    "max_iters = 2000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 150\n",
    "n_embd = 256\n",
    "n_head = 8\n",
    "n_layer = 4\n",
    "dropout = 0.3\n",
    "# ------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTLanguageModel(\n",
       "  (token_embedding_table): Embedding(14310, 256)\n",
       "  (position_embedding_table): Embedding(128, 256)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (key): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (query): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (value): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.3, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (3): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (key): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (query): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (value): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.3, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (3): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (key): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (query): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (value): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.3, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (3): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (key): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (query): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (value): Linear(in_features=256, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.3, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (3): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): Linear(in_features=256, out_features=14310, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPTLanguageModel()\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! changes fruit-trees doing wronging undergo sky Dull inveterate shin December modesties hazards Wiltshire Peer bisson lasted Poland mourning naught whined Beating rascal Bloody Temperance Mercutio join prays potent uncontroll Benefactors pouring tides 'Bove Consuming freezes professors c. minim Exeter advancement presses unrest offends poor'st debile lazy Gentle Thanks Are purchase standard whilst hewn Drinking pushed FITZWATER plagued colic patrimony Comest Claudio Romans unloose cities heat alone whipp hideous coining laments Tabours mix slaughter-house Behind dun nostril thundering licence ill-beseeming caetera hereditary court'sies gratulate Didst irons Else suspected strong meteor Whereby unreverent stretches praying told grossness Caliban knowest thickest YORK Finding tread lying presumes CATESBY absolute story Phoebus thriftless Englishman Care delights exceeds Angelica affairs wisely breasted forecast crutches Patrician mulberry ashes forecast mistaking consumed Aufidiuses 'zounds provide redeems rooteth meal park-corner home diadem attempted grandmother howled top-branch Tire Richmond cowardly o'ershade Show Bohemia wilful-negligent anguish Pisa chattering penalty travell Pierce merciless recovered o'erthrow live weigh purity shepherdess chapel guide I'faith caetera tail Worshipful well-govern stop what blended chains spited ambassador repay bewitchment Consenting Is suburbs ripe mangle scruple unpeople bridal footstool Neither durance butcher Sharp Truly shield greediness loins greet adjacent Bohemian pennyworth wonderful rice dulness accident applause evidence sorrow betide Abbot pair Subtly industriously woolvish heavy language immediate worthiness neb cheques rood Forbiddenly Pewter able forget'st unscann shooting poltroons ADRIAN holp sever Griefs aboding ditch sued banks impregnable oddest won sooth resolute Advantaging shoulder-shotten Clarence wars breed possessed uncrown violently engaged brood phrase O'ercome tomorrow rapier a-high remiss antique commenting bethought Methinks attorneyed Titus dealt adorns hoodwink devices pestering shut assisting wrecked pleasures shoulder-bone intolerable Florence laboured rigour Roundly sheep-whistling slug extremes dearer baked Holding upbraidings show't memory Christendom rooms midst bawds love Ross accomplished roots mutton VINCENTIO perjured honestly is discredited tribes societies Balk Supper curse matters greatest other giant offer'd microcosm hopeful employments mild pricking glowing breeches playing haste Fellows Governs ways Sea-water misbehaved humanely pleader gnarling liking Poor wake fan sufferance bosoms Land charge stupid stand misled agile Slaughters metaphysics Unbind Stir 'Come Tire o'ershades Ho infold Dispark footman powerful compare expedient vestal Rich meet Gremio turning befell perjured stories usest blood-suckers royalties fathom overheard'st pine unthrifts beauty Stoop cushion benefactors tenth Mercury unreasonably beguile dregs presumes AEdiles history There 'Zounds halbert crafted buzzed Pomfret kneaded dribbling revenge satin spake spend alehouse ROMEO deserves lozel Hiding fantastical Conceit mortal faster edicts gout works Forewarn honesty whoremaster amber Shore darken swear'st morrow copatain ring patiently hungerly debts wot torch-bearer impose lyingest Curtis confines curtain sizes broke womanly ruffian prop inferior allegiance dissemblers death hoar Sleep brook invisible Peter privy leaves Ourself sensible rice wait Excepting counts importance unswayable injured dogs preserving Content trained insatiate unelected learn'd troublest Switch balm Aurora laments bonneted chamberlain grew redeliver dew LORDS jacks Sinfully swarm Bretagne kin watching quailing glisters perceived crafted betray man attorneyed languish criminal 'lady slander clouded politicly accoutrements discovery tear senseless Fan barne LEWIS dry-beat golden Shed unite unstain mulberry divers Bate settled judgment Address ancestor lamentable wither'd\n"
     ]
    }
   ],
   "source": [
    "print(decode(model.generate(idx=torch.zeros((1, 1), dtype=torch.long, device=DEVICE), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [05:50<00:00, 28.53it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 10000\n",
    "losses = []\n",
    "for e in tqdm(range(epochs)):\n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! Dove-feather it tongue Against all this substance of government , which in his princely death ! DERBY : It makes him in hell : O , methoughts , look worn , a mightst be out of a fearful eyes , A pair of peace and fall , a burthen This good ones and where I slip away with you am sold , To comfort To bitterest enmity ; no , But hear some trust what time , we 'll speak free . First , 't is thy queen of such a manifested ; but whom I will advise myself .\n"
     ]
    }
   ],
   "source": [
    "print(decode(model.generate(idx=torch.zeros((1, 1), dtype=torch.long, device=DEVICE), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
