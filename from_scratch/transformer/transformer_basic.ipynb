{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1da9d545830>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/input.txt', 'r', encoding='utf-8') as f:\n",
    "    shakespeare = f.read()\n",
    "    \n",
    "# List all unique characters that occurs in the input text\n",
    "chars = sorted(list(set(shakespeare)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# mapping from characters to integers for encoding\n",
    "stoi = { ch:i for i, ch in enumerate(chars) }\n",
    "itos = { i:ch for i, ch in enumerate(chars) }\n",
    "encode = lambda s : [stoi[c] for c in s] # Take a string and output a list of integers\n",
    "decode = lambda i : ''.join([itos[c] for c in i]) # Take a list of integers and output a list of string\n",
    "\n",
    "# encode the entire text dataset and store it into a torch.Tensor\n",
    "data = torch.tensor(encode(shakespeare), dtype=torch.long)\n",
    "\n",
    "# Let's now split up the data into train and validation sets\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # How many independent sequences will be process in parallel?\n",
    "block_size = 64 # What is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x.to(device=DEVICE), y.to(device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" One head of self-attention \"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd, block_size, head_size) -> None:\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril((torch.ones(block_size, block_size))))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        # Compute the attention score using k and q \n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        w = q @ k.transpose(-2, -1) * C ** -0.5\n",
    "        w = w.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        w = F.softmax(w, dim=-1)\n",
    "        \n",
    "        # weight aggregation of the values using the attention score\n",
    "        v = self.value(x)\n",
    "        out = w @ v\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd, block_size, head_size, n_head, dropout):\n",
    "        super().__init__()\n",
    "        self.head = nn.ModuleList([Head(n_embd, block_size, head_size) for _ in range(n_head)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Use multiple heads of self-attention \n",
    "        # Concatanate each output alone the embedding dimension (B, T ,C), that is merged alone C\n",
    "        out = torch.cat([h(x) for h in self.head], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\" A simple linear layer followed by a non-linearity \"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd, dropout) -> None:\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "                    nn.Linear(n_embd, 4 * n_embd),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(4 * n_embd, n_embd),\n",
    "                    nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnBlock(nn.Module):\n",
    "    def __init__(self, n_embd, block_size, n_head, dropout) -> None:\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.attn = MultiHeadAttention(n_embd, block_size, head_size, n_head, dropout)\n",
    "        self.ffn = FeedForward(n_embd, dropout)\n",
    "        self.ln = nn.LayerNorm(n_embd)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Use the residual connection with layernorm\n",
    "        x = x + self.attn(self.ln(x))\n",
    "        x = x + self.ffn(self.ln(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A modified neural network for bigram model\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd, block_size, n_head, n_layer, dropout) -> None:\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.attn_blocks = nn.Sequential(*[AttnBlock(n_embd, block_size, n_head, dropout) for _ in range(n_layer)])\n",
    "        self.ln = nn.LayerNorm(n_embd)\n",
    "        self.ln_head = nn.Linear(n_embd, vocab_size)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx) # (B, T, n_embd)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=DEVICE))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.attn_blocks(x)\n",
    "        x = self.ln(x)\n",
    "        logits = self.ln_head(x)            \n",
    "        \n",
    "        if targets is None: # for generation without providing target\n",
    "            loss = None\n",
    "        else:\n",
    "            # Reshape the logits tensor to meet definition of the cross_entropy function in Pytorch\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C) # Concatenate B, T\n",
    "            targets = targets.view(B*T)    # Same reshaping to the target tensor\n",
    "            loss = F.cross_entropy(logits, targets) # calculate the loss\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generation(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens to meet position embedding range\n",
    "            idx_crop = idx[:, -self.block_size:]\n",
    "            \n",
    "            # get the predictions\n",
    "            logits, _ = self(idx_crop)\n",
    "            \n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            \n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            \n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            \n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "            \n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (token_embedding_table): Embedding(65, 32)\n",
       "  (position_embedding_table): Embedding(64, 32)\n",
       "  (attn_blocks): Sequential(\n",
       "    (0): AttnBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (head): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=32, out_features=8, bias=False)\n",
       "            (query): Linear(in_features=32, out_features=8, bias=False)\n",
       "            (value): Linear(in_features=32, out_features=8, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=128, out_features=32, bias=True)\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): AttnBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (head): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=32, out_features=8, bias=False)\n",
       "            (query): Linear(in_features=32, out_features=8, bias=False)\n",
       "            (value): Linear(in_features=32, out_features=8, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=128, out_features=32, bias=True)\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): AttnBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (head): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=32, out_features=8, bias=False)\n",
       "            (query): Linear(in_features=32, out_features=8, bias=False)\n",
       "            (value): Linear(in_features=32, out_features=8, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=128, out_features=32, bias=True)\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): AttnBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (head): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=32, out_features=8, bias=False)\n",
       "            (query): Linear(in_features=32, out_features=8, bias=False)\n",
       "            (value): Linear(in_features=32, out_features=8, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=128, out_features=32, bias=True)\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (4): AttnBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (head): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=32, out_features=8, bias=False)\n",
       "            (query): Linear(in_features=32, out_features=8, bias=False)\n",
       "            (value): Linear(in_features=32, out_features=8, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=128, out_features=32, bias=True)\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (5): AttnBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (head): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=32, out_features=8, bias=False)\n",
       "            (query): Linear(in_features=32, out_features=8, bias=False)\n",
       "            (value): Linear(in_features=32, out_features=8, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffn): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=128, out_features=32, bias=True)\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (ln): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "  (ln_head): Linear(in_features=32, out_features=65, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transformer(vocab_size, n_embd=32, block_size=block_size, n_head=4, n_layer=6, dropout=0.0)\n",
    "model.to(device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL params num: 81601\n"
     ]
    }
   ],
   "source": [
    "# params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "params = sum(p.numel() for p in model.parameters())\n",
    "print(f'TOTAL params num: {params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [01:49<00:00, 45.50it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 5000\n",
    "losses = []\n",
    "for e in tqdm(range(epochs)):\n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7031662464141846\n"
     ]
    }
   ],
   "source": [
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MOPEES:\n",
      "Power grivent I why myself have mysel--'Kome.\n",
      "\n",
      "Gentlematan:\n",
      "What shall so not us bod, whom.\n",
      "\n",
      "My some should sold. A En shondluce.\n",
      "\n",
      "Serself Our be thou tend?\n",
      "Carst to Pesirk ovet some so upon sixe fear.\n",
      "As not come, not sun?\n",
      "\n",
      "Your lowfUere, soxe are and barakes again\n",
      "Abucces my poserfore long my doscelly no crive,\n",
      "Tcleadins, his I it him.\n",
      "\n",
      "GLOUCUSTERCE:\n",
      "Fyreions, to meimed the no stee summe wip.\n",
      "\n",
      "GLOUCEMELA:\n",
      "Which brukle Enjeporn man I so hander\n",
      "Sibe make chook Yow their forcesy, know our\n"
     ]
    }
   ],
   "source": [
    "print(decode(model.generation(idx=torch.zeros((1, 1), dtype=torch.long, device=DEVICE), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size_str = str(params/1e6)\n",
    "model_file_name = 'transformer' + model_size_str + 'M.pth'\n",
    "model_save_path = '../params/transformer/' + model_file_name\n",
    "torch.save(model.state_dict(), model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model = Transformer(vocab_size, n_embd=32, block_size=block_size, n_head=4, n_layer=6, dropout=0.0).to(DEVICE)\n",
    "trained_model.load_state_dict(torch.load('../params/transformer/transformer0.081601M.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Miser, I blanives have to ackin, Clombeing how\n",
      "and, good atce\n",
      "It have druess? Do nobnow.\n",
      "\n",
      "Find, I knothged the shall thou fries thank be craise.\n",
      "\n",
      "My Somenor of Gloundence.\n",
      "\n",
      "QUEEA:\n",
      "I in canise on cloil him.\n",
      "\n",
      "JULIET:\n",
      "Which well, sir did hrother what stendemisss Plencuaic;\n",
      "When sab here but have to muttan'd,\n",
      "Let Blovege yermbeich bloing here.\n",
      "\n",
      "ixcluver:\n",
      "Go no\n",
      "Cleapt my Glorain:\n",
      "Lethey corthall, hust in by Clrest to brumblel,\n",
      "Her crince you'lk'\n",
      "the sby love yhalt he jroyal\n",
      "him, thing wear wonl is t\n"
     ]
    }
   ],
   "source": [
    "print(decode(trained_model.generation(idx=torch.zeros((1, 1), dtype=torch.long, device=DEVICE), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
