{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download() # if not install the nltk library then uncomment this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'I love this flavor! It\\'s by far the best choice and my go-to whenever I go to the grocery store. I wish they would restock it more often though.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'love', 'this', 'flavor', '!', 'It', \"'s\", 'by', 'far', 'the', 'best', 'choice', 'and', 'my', 'go-to', 'whenever', 'I', 'go', 'to', 'the', 'grocery', 'store', '.', 'I', 'wish', 'they', 'would', 'restock', 'it', 'more', 'often', 'though', '.']\n"
     ]
    }
   ],
   "source": [
    "word_tokens = word_tokenize(text)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14310"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = sorted(list(set(word_tokenize(text))))\n",
    "vocab_size = len(words)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { w:i for i,w in enumerate(words) }\n",
    "itos = { i:w for i,w in enumerate(words) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ' '.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3053, 3512, 3324, 11053, 10791, 13010, 5723, 12819, 13010, 6533, 225]\n",
      "You are all resolved rather to die than to famish ?\n"
     ]
    }
   ],
   "source": [
    "test_string = 'You are all resolved rather to die than to famish?'\n",
    "print(encode(word_tokenize(test_string)))\n",
    "print(decode(encode(word_tokenize(test_string))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([254509]) <built-in method type of Tensor object at 0x00000216A87145E0>\n",
      "tensor([ 1152,   709,   223,   482, 13877, 10480,  3440,  7080,   219,  7604,\n",
      "         8993, 12087,   221,   323,   223,  2520,   219, 12087,   221,  1152,\n",
      "          709,   223,  3053,  3512,  3324, 11053, 10791, 13010,  5723, 12819,\n",
      "        13010,  6533,   225,   323,   223,  2256,   221, 11053,   221,  1152,\n",
      "          709,   223,  1152,   219, 14291,  8402,   640,  1769,  8232,  4679,\n",
      "         6251, 13010, 12831, 10036,   221,   323,   223,  2919,  8404,   219,\n",
      "        13877,  8404,   221,  1152,   709,   223,  1679, 13581,  8340,  7738,\n",
      "          219,  3412, 13877,   162,  7567,  5147,  3596,  9761,  9833, 10439,\n",
      "          221,  1547,  3061, 13661,   225,   323,   223,  1924,  9271, 12703,\n",
      "         9583,  9377,   224,  8580,  8243,  3786,  5952,   223,  3659,   219,\n",
      "         3659,     0,  2370,   709,   223,  1972, 14183,   219,  7277,  4738,\n",
      "          221,  1152,   709,   223,  2919,  3512,  3145, 10283,  4738,   219,\n",
      "        12831,  9969,  7277,   221,  2943,  3643, 12582,  9697, 14214, 10931,\n",
      "        13581,   223,  7937, 12857, 14214, 14282, 13581,  4389, 12831, 12548,\n",
      "          219, 14003,  8243, 13966, 14029,   219, 13877,  9103,  7425, 12857,\n",
      "        10932, 13581,  7886,   224,  4389, 12857, 12873, 13877,  3512, 13038,\n",
      "         5479,   223, 12831,  8529, 12828,  3257, 13581,   219, 12831,  9629,\n",
      "         9659,  9761,  9181,   219,  8232,  3552,  3400,  8211, 13010,  9928,\n",
      "        12835,  3115,   224,  9761, 12505,  8232,  3061,  7090, 13010, 12837,\n",
      "         1679, 13581, 11097, 12883, 14121,  9761, 10136,   219,  6329, 13877,\n",
      "         3827, 10753,   223,  6877, 12831,  7269,  8402,  1478, 12087, 12883,\n",
      "         8017,  7898,  6877,  4245,   219,  9545,  8017, 12880,  6877, 11097,\n",
      "          221,  2370,   709,   223,  3027, 14291, 10480,  6338,  3277,   640,\n",
      "         1769,   225,   323,   223,   301,  7738,  6732,   223,  7585,   191,\n",
      "         3061, 13671,  5936, 13010, 12831,  4901,   221,  2370,   709,   223,\n",
      "          778, 14291, 13973, 11572,  7585,  7541,  5952,  6877,  7748,  5203,\n",
      "          225,  1152,   709,   223,  2870, 13943,   224,  3412,  5176,  3786,\n",
      "         5078, 13010,  7211,  7738,  7277, 10999,  6946,   219,  4389, 12828,\n",
      "         7585,  9984,  7739, 14121,  3906, 10582,   221,  2370,   709,   223,\n",
      "         1895,   219,  4389, 12087,  9545,  8879,   221,  1152,   709,   223,\n",
      "         1478, 11365, 13533, 14291,   219, 13973,  7585,  7559,  5952,  6535,\n",
      "          219,  7585,  5721,  8243, 13010, 12828,  6236,   223, 12894, 11986,\n",
      "         9047,  4455,  3786,  5078, 13010, 11365,  8243, 13840,  6877,  7748,\n",
      "         5203,  7585,  5721,  8243, 13010, 10224,  7748,  9284,  3412, 13010,\n",
      "         3786,  9934, 10582,   224, 14002,  7585,  8232,   219,  6359, 12982,\n",
      "        12831,  3361,  9659,  7748, 13727,   221,  2370,   709,   223,  2943,\n",
      "         7585,  4455,  9545,  7673,  8017,  7748,  9400,   219, 14291,  3143,\n",
      "         3061, 13684,  8017,  7738,   221,  3053,  9356,  8017,  9510, 13874,\n",
      "        11365,  7585,  8232,  5243,   221,  1152,   709,   223,  1488,  1478,\n",
      "         9356,  9545,   219,  1478,  9426,  9545,  3786,  3745,  9659,  3150,\n",
      "          224,  7585,  7559,  6583,   219, 14121, 12592,   219, 13010, 13000,\n",
      "         8017, 10990,   221,  2943, 11721,  3512, 12856,   225,  2661,  9756,\n",
      "        11770,  9583,     3, 12831,  4739,  8232, 11168,   223, 14042, 12263,\n",
      "        13877, 10353,  7694,   225, 13010, 12831,   659,     0,   323,   223,\n",
      "          735,   219,  4862,   221,  1152,   709,   223,  2488,     0, 14025,\n",
      "         4866,  7694,   225,  2370,   709,   223,  3024,  1815,   307,   224,\n",
      "         9701, 12828,  7559,  3364,  8775, 12831, 10036,   221,  1152,   709,\n",
      "          223,  1376,   191,  9701,  7793,  6277,   223, 14214,  3324, 12831,\n",
      "        11064, 13966, 11970,     0,  1740,   223,  2943, 14186,   191,   219,\n",
      "         9372,  5205,   219,  8017,  7481,   225, 13984,  7253, 14291,  2999,\n",
      "         3769,  3412,  4802,   225,  2661,  8981,   225, 12087,   219,  1478,\n",
      "        10356, 14291,   221,  1152,   709,   223,  1981,  4384,  8232,  9545,\n",
      "        13422, 13010, 12831, 11530,   224, 12857,  7567,  7453,  8122, 12883,\n",
      "         6953, 13973, 13877,  8174, 13010,  5928,   219, 14002,  9564, 13877,\n",
      "          162, 11723,   134,  8017,  5528,   221,  2673, 11365, 10283, 12520,\n",
      "         7567, 12428,  4264,   223, 12857, 11615,  8402, 13877,  7567, 12428,\n",
      "         3530, 13038,   221,  1740,   223,  2981,   219,  8969,   219,  9372,\n",
      "         7277,  7026,   219,  9132,  7793,  9446,   219,  2984, 14291, 13358,\n",
      "        14303,   225,  1152,   709,   223,  2919,  4455,  9545,   219, 11835,\n",
      "          219, 13877,  3512, 13360,  3353,   221,  1740,   223,  1478, 12761,\n",
      "        14291,   219,  7026,   219,  9283,  4625,  4496,  1374, 12831,  9969,\n",
      "         9659, 14291,   221,  1180, 14300, 13813,   219,  3058, 12507,  8017,\n",
      "        12883,  5487,   219, 14291,  8986,  3552, 13943,  2568,  3596, 12831,\n",
      "         7633, 14121, 14300, 12262,  3552,  8624, 12837,   301, 12831,  2291,\n",
      "        12254,   219, 14038,  5212, 14065,  9697,  2661, 13874,  8243, 12693,\n",
      "          219,  5263, 12785, 12897,  5360,  1958,  9271, 12428,  8669,  3595,\n",
      "        12819,  4455,  6363,   373,  8017, 14300,  7982,   221,  1180, 12831,\n",
      "         5487,   219,  2661,  7269,   219,  9545, 12831,  9969,   219,  8863,\n",
      "         8243,   219,  3412,  3058,  8387, 13010, 12837,   219,  9545,  3530,\n",
      "          219,  9356,  7673,   221,   314,   219,  3053,  3512, 13114,  4412,\n",
      "         4429,  2684, 13984,  9271,  3615, 14291,   219,  3412, 14291, 11872,\n",
      "         2661,  7672,  9583,     3, 12831, 12254,   219, 14025,  4496,  6877,\n",
      "        14291,  8636,  6576,   219,  2946, 14291,  5376, 12837,  3552,  6250,\n",
      "          221,  1152,   709,   223,   664,  6877, 13581,     0,  2763,   219,\n",
      "         8049,     0,  2673,  9408,  4499,  6877, 13581, 14278,   223, 12504,\n",
      "        13581, 13010,  6533,   219,  3412, 12835, 12353,  5274, 14121,  7315,\n",
      "          224,  8863,  6158,  6877, 13600,   219, 13010, 12561, 13591,   224,\n",
      "        10984,  5404,  3440, 14029,  3173,  6346,  3277, 12831, 11129,   219,\n",
      "         3412, 10596,  9271, 10133, 12261,  5404,   219, 13010,  4587, 13559,\n",
      "         3412, 11074, 12831, 10283,   221,  1488, 12831, 13838,  6146, 13581,\n",
      "         9545, 13559,   219, 12857, 14065,   224,  3412, 12843,   191,  3324,\n",
      "        12831,  8770, 12857,  3796, 13581,   221,  1740,   223,  1020, 14291,\n",
      "         9356,   768, 14303, 14168,  8878,   219,  1979,  3786,  3152,  9659,\n",
      "         6856,   221,  1478, 11615, 12761, 14291,   226, 10429, 12697,   223,\n",
      "         8243,  8986,  3786, 14291,  7567,  7607,  8243,   224,   618,   219,\n",
      "        11807,  8243, 11569,  9372, 10654,   219,  1478, 14065, 13658,  2722,\n",
      "        12221,   202,  3061,  8684,  9271,   221,  1152,   709,   223,  2930,\n",
      "          219,  1478,   162,  7604,  8243,   219, 11835,   223, 14278, 14291,\n",
      "         9356,  9545, 12873, 13010,  6834,  9660,  9761,  5813, 14121,  3061,\n",
      "        12697,   223,  4389,   219,  3400,   202, 10224, 14291,   219,  5582,\n",
      "          221,  1740,   223,  2665, 13840,  3061, 12986, 13981,  3324, 12831,\n",
      "         4132,   191,  9045,  2214,   130,  3277, 12831,  3916,   219, 12963,\n",
      "         3152,  8243,   223,  2660,  9704,  8636,  3061,  7439,  8243,  5721,\n",
      "        10938,  1478,     3, 12831,  9101,  9583,     3, 12831,  4132,   219,\n",
      "         7932,  3412, 13293,   219,  2556,  5356, 12831, 13681,   219,  9460,\n",
      "         3804,  1693,  8414, 14121, 12831, 11064,   219, 13984, 12831,  9756,\n",
      "         8161,   919, 11488,  3412,  7604,   219,  5698,   219,  8155,   219,\n",
      "        13793,   219,  6626,   219,   350,   219,  9370,  9926,   219,  5721,\n",
      "         9139,  2833, 12831,  3473,  3412,  3249,  4899,  1958, 12831, 14028,\n",
      "         4132,   221,  2661,  3916,  3430,   130,   220,  1152,   709,   223])\n"
     ]
    }
   ],
   "source": [
    "# encode the entire text dataset and store it into a torch.Tensor\n",
    "data = torch.tensor(encode(word_tokenize(text)), dtype=torch.long)\n",
    "print(data.shape, data.type)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now split up the data into train and validation sets\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1152,   709,   223,   482, 13877, 10480,  3440,  7080,   219])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: tensor([1152]), target: 709\n",
      "input: tensor([1152,  709]), target: 223\n",
      "input: tensor([1152,  709,  223]), target: 482\n",
      "input: tensor([1152,  709,  223,  482]), target: 13877\n",
      "input: tensor([ 1152,   709,   223,   482, 13877]), target: 10480\n",
      "input: tensor([ 1152,   709,   223,   482, 13877, 10480]), target: 3440\n",
      "input: tensor([ 1152,   709,   223,   482, 13877, 10480,  3440]), target: 7080\n",
      "input: tensor([ 1152,   709,   223,   482, 13877, 10480,  3440,  7080]), target: 219\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f'input: {context}, target: {target}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # How many independent sequences will be process in parallel?\n",
    "block_size = 8 # What is the maximum context length for predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[ 1478,  8532,  9659, 14291,   219,   392,  9701, 12828],\n",
      "        [ 1478,   162,  9460,  9975,  3276,   219,  9460, 12228],\n",
      "        [  223,   391, 12891,  3061,  8889,   225, 12968,  6928],\n",
      "        [ 8348,  8017,  7560,   223,  2943,  4872,  7857,  4468]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[ 8532,  9659, 14291,   219,   392,  9701, 12828,  3512],\n",
      "        [  162,  9460,  9975,  3276,   219,  9460, 12228, 12306],\n",
      "        [  391, 12891,  3061,  8889,   225, 12968,  6928,  5304],\n",
      "        [ 8017,  7560,   223,  2943,  4872,  7857,  4468, 12891]])\n"
     ]
    }
   ],
   "source": [
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1/4\n",
      "when input is [1478] the target: 8532\n",
      "when input is [1478, 8532] the target: 9659\n",
      "when input is [1478, 8532, 9659] the target: 14291\n",
      "when input is [1478, 8532, 9659, 14291] the target: 219\n",
      "when input is [1478, 8532, 9659, 14291, 219] the target: 392\n",
      "when input is [1478, 8532, 9659, 14291, 219, 392] the target: 9701\n",
      "when input is [1478, 8532, 9659, 14291, 219, 392, 9701] the target: 12828\n",
      "when input is [1478, 8532, 9659, 14291, 219, 392, 9701, 12828] the target: 3512\n",
      "\n",
      "batch 2/4\n",
      "when input is [1478] the target: 162\n",
      "when input is [1478, 162] the target: 9460\n",
      "when input is [1478, 162, 9460] the target: 9975\n",
      "when input is [1478, 162, 9460, 9975] the target: 3276\n",
      "when input is [1478, 162, 9460, 9975, 3276] the target: 219\n",
      "when input is [1478, 162, 9460, 9975, 3276, 219] the target: 9460\n",
      "when input is [1478, 162, 9460, 9975, 3276, 219, 9460] the target: 12228\n",
      "when input is [1478, 162, 9460, 9975, 3276, 219, 9460, 12228] the target: 12306\n",
      "\n",
      "batch 3/4\n",
      "when input is [223] the target: 391\n",
      "when input is [223, 391] the target: 12891\n",
      "when input is [223, 391, 12891] the target: 3061\n",
      "when input is [223, 391, 12891, 3061] the target: 8889\n",
      "when input is [223, 391, 12891, 3061, 8889] the target: 225\n",
      "when input is [223, 391, 12891, 3061, 8889, 225] the target: 12968\n",
      "when input is [223, 391, 12891, 3061, 8889, 225, 12968] the target: 6928\n",
      "when input is [223, 391, 12891, 3061, 8889, 225, 12968, 6928] the target: 5304\n",
      "\n",
      "batch 4/4\n",
      "when input is [8348] the target: 8017\n",
      "when input is [8348, 8017] the target: 7560\n",
      "when input is [8348, 8017, 7560] the target: 223\n",
      "when input is [8348, 8017, 7560, 223] the target: 2943\n",
      "when input is [8348, 8017, 7560, 223, 2943] the target: 4872\n",
      "when input is [8348, 8017, 7560, 223, 2943, 4872] the target: 7857\n",
      "when input is [8348, 8017, 7560, 223, 2943, 4872, 7857] the target: 4468\n",
      "when input is [8348, 8017, 7560, 223, 2943, 4872, 7857, 4468] the target: 12891\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for b in range(batch_size): # batch dimension\n",
    "    print(f'batch {b+1}/{batch_size}')\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
