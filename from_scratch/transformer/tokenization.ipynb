{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download() # if not install the nltk library then uncomment this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'I love this flavor! It\\'s by far the best choice and my go-to whenever I go to the grocery store. I wish they would restock it more often though.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'love', 'this', 'flavor', '!', 'It', \"'s\", 'by', 'far', 'the', 'best', 'choice', 'and', 'my', 'go-to', 'whenever', 'I', 'go', 'to', 'the', 'grocery', 'store', '.', 'I', 'wish', 'they', 'would', 'restock', 'it', 'more', 'often', 'though', '.']\n"
     ]
    }
   ],
   "source": [
    "word_tokens = word_tokenize(text)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14310"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = sorted(list(set(word_tokenize(text))))\n",
    "vocab_size = len(words)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { w:i for i,w in enumerate(words) }\n",
    "itos = { i:w for i,w in enumerate(words) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ' '.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3053, 3512, 3324, 11053, 10791, 13010, 5723, 12819, 13010, 6533, 225]\n",
      "You are all resolved rather to die than to famish ?\n"
     ]
    }
   ],
   "source": [
    "test_string = 'You are all resolved rather to die than to famish?'\n",
    "print(encode(word_tokenize(test_string)))\n",
    "print(decode(encode(word_tokenize(test_string))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([254509]) <built-in method type of Tensor object at 0x000001B072FA8C20>\n",
      "tensor([ 1152,   709,   223,   482, 13877, 10480,  3440,  7080,   219,  7604,\n",
      "         8993, 12087,   221,   323,   223,  2520,   219, 12087,   221,  1152,\n",
      "          709,   223,  3053,  3512,  3324, 11053, 10791, 13010,  5723, 12819,\n",
      "        13010,  6533,   225,   323,   223,  2256,   221, 11053,   221,  1152,\n",
      "          709,   223,  1152,   219, 14291,  8402,   640,  1769,  8232,  4679,\n",
      "         6251, 13010, 12831, 10036,   221,   323,   223,  2919,  8404,   219,\n",
      "        13877,  8404,   221,  1152,   709,   223,  1679, 13581,  8340,  7738,\n",
      "          219,  3412, 13877,   162,  7567,  5147,  3596,  9761,  9833, 10439,\n",
      "          221,  1547,  3061, 13661,   225,   323,   223,  1924,  9271, 12703,\n",
      "         9583,  9377,   224,  8580,  8243,  3786,  5952,   223,  3659,   219,\n",
      "         3659,     0,  2370,   709,   223,  1972, 14183,   219,  7277,  4738,\n",
      "          221,  1152,   709,   223,  2919,  3512,  3145, 10283,  4738,   219,\n",
      "        12831,  9969,  7277,   221,  2943,  3643, 12582,  9697, 14214, 10931,\n",
      "        13581,   223,  7937, 12857, 14214, 14282, 13581,  4389, 12831, 12548,\n",
      "          219, 14003,  8243, 13966, 14029,   219, 13877,  9103,  7425, 12857,\n",
      "        10932, 13581,  7886,   224,  4389, 12857, 12873, 13877,  3512, 13038,\n",
      "         5479,   223, 12831,  8529, 12828,  3257, 13581,   219, 12831,  9629,\n",
      "         9659,  9761,  9181,   219,  8232,  3552,  3400,  8211, 13010,  9928,\n",
      "        12835,  3115,   224,  9761, 12505,  8232,  3061,  7090, 13010, 12837,\n",
      "         1679, 13581, 11097, 12883, 14121,  9761, 10136,   219,  6329, 13877,\n",
      "         3827, 10753,   223,  6877, 12831,  7269,  8402,  1478, 12087, 12883,\n",
      "         8017,  7898,  6877,  4245,   219,  9545,  8017, 12880,  6877, 11097,\n",
      "          221,  2370,   709,   223,  3027, 14291, 10480,  6338,  3277,   640,\n",
      "         1769,   225,   323,   223,   301,  7738,  6732,   223,  7585,   191,\n",
      "         3061, 13671,  5936, 13010, 12831,  4901,   221,  2370,   709,   223,\n",
      "          778, 14291, 13973, 11572,  7585,  7541,  5952,  6877,  7748,  5203,\n",
      "          225,  1152,   709,   223,  2870, 13943,   224,  3412,  5176,  3786,\n",
      "         5078, 13010,  7211,  7738,  7277, 10999,  6946,   219,  4389, 12828,\n",
      "         7585,  9984,  7739, 14121,  3906, 10582,   221,  2370,   709,   223,\n",
      "         1895,   219,  4389, 12087,  9545,  8879,   221,  1152,   709,   223,\n",
      "         1478, 11365, 13533, 14291,   219, 13973,  7585,  7559,  5952,  6535,\n",
      "          219,  7585,  5721,  8243, 13010, 12828,  6236,   223, 12894, 11986,\n",
      "         9047,  4455,  3786,  5078, 13010, 11365,  8243, 13840,  6877,  7748,\n",
      "         5203,  7585,  5721,  8243, 13010, 10224,  7748,  9284,  3412, 13010,\n",
      "         3786,  9934, 10582,   224, 14002,  7585,  8232,   219,  6359, 12982,\n",
      "        12831,  3361,  9659,  7748, 13727,   221,  2370,   709,   223,  2943,\n",
      "         7585,  4455,  9545,  7673,  8017,  7748,  9400,   219, 14291,  3143,\n",
      "         3061, 13684,  8017,  7738,   221,  3053,  9356,  8017,  9510, 13874,\n",
      "        11365,  7585,  8232,  5243,   221,  1152,   709,   223,  1488,  1478,\n",
      "         9356,  9545,   219,  1478,  9426,  9545,  3786,  3745,  9659,  3150,\n",
      "          224,  7585,  7559,  6583,   219, 14121, 12592,   219, 13010, 13000,\n",
      "         8017, 10990,   221,  2943, 11721,  3512, 12856,   225,  2661,  9756,\n",
      "        11770,  9583,     3, 12831,  4739,  8232, 11168,   223, 14042, 12263,\n",
      "        13877, 10353,  7694,   225, 13010, 12831,   659,     0,   323,   223,\n",
      "          735,   219,  4862,   221,  1152,   709,   223,  2488,     0, 14025,\n",
      "         4866,  7694,   225,  2370,   709,   223,  3024,  1815,   307,   224,\n",
      "         9701, 12828,  7559,  3364,  8775, 12831, 10036,   221,  1152,   709,\n",
      "          223,  1376,   191,  9701,  7793,  6277,   223, 14214,  3324, 12831,\n",
      "        11064, 13966, 11970,     0,  1740,   223,  2943, 14186,   191,   219,\n",
      "         9372,  5205,   219,  8017,  7481,   225, 13984,  7253, 14291,  2999,\n",
      "         3769,  3412,  4802,   225,  2661,  8981,   225, 12087,   219,  1478,\n",
      "        10356, 14291,   221,  1152,   709,   223,  1981,  4384,  8232,  9545,\n",
      "        13422, 13010, 12831, 11530,   224, 12857,  7567,  7453,  8122, 12883,\n",
      "         6953, 13973, 13877,  8174, 13010,  5928,   219, 14002,  9564, 13877,\n",
      "          162, 11723,   134,  8017,  5528,   221,  2673, 11365, 10283, 12520,\n",
      "         7567, 12428,  4264,   223, 12857, 11615,  8402, 13877,  7567, 12428,\n",
      "         3530, 13038,   221,  1740,   223,  2981,   219,  8969,   219,  9372,\n",
      "         7277,  7026,   219,  9132,  7793,  9446,   219,  2984, 14291, 13358,\n",
      "        14303,   225,  1152,   709,   223,  2919,  4455,  9545,   219, 11835,\n",
      "          219, 13877,  3512, 13360,  3353,   221,  1740,   223,  1478, 12761,\n",
      "        14291,   219,  7026,   219,  9283,  4625,  4496,  1374, 12831,  9969,\n",
      "         9659, 14291,   221,  1180, 14300, 13813,   219,  3058, 12507,  8017,\n",
      "        12883,  5487,   219, 14291,  8986,  3552, 13943,  2568,  3596, 12831,\n",
      "         7633, 14121, 14300, 12262,  3552,  8624, 12837,   301, 12831,  2291,\n",
      "        12254,   219, 14038,  5212, 14065,  9697,  2661, 13874,  8243, 12693,\n",
      "          219,  5263, 12785, 12897,  5360,  1958,  9271, 12428,  8669,  3595,\n",
      "        12819,  4455,  6363,   373,  8017, 14300,  7982,   221,  1180, 12831,\n",
      "         5487,   219,  2661,  7269,   219,  9545, 12831,  9969,   219,  8863,\n",
      "         8243,   219,  3412,  3058,  8387, 13010, 12837,   219,  9545,  3530,\n",
      "          219,  9356,  7673,   221,   314,   219,  3053,  3512, 13114,  4412,\n",
      "         4429,  2684, 13984,  9271,  3615, 14291,   219,  3412, 14291, 11872,\n",
      "         2661,  7672,  9583,     3, 12831, 12254,   219, 14025,  4496,  6877,\n",
      "        14291,  8636,  6576,   219,  2946, 14291,  5376, 12837,  3552,  6250,\n",
      "          221,  1152,   709,   223,   664,  6877, 13581,     0,  2763,   219,\n",
      "         8049,     0,  2673,  9408,  4499,  6877, 13581, 14278,   223, 12504,\n",
      "        13581, 13010,  6533,   219,  3412, 12835, 12353,  5274, 14121,  7315,\n",
      "          224,  8863,  6158,  6877, 13600,   219, 13010, 12561, 13591,   224,\n",
      "        10984,  5404,  3440, 14029,  3173,  6346,  3277, 12831, 11129,   219,\n",
      "         3412, 10596,  9271, 10133, 12261,  5404,   219, 13010,  4587, 13559,\n",
      "         3412, 11074, 12831, 10283,   221,  1488, 12831, 13838,  6146, 13581,\n",
      "         9545, 13559,   219, 12857, 14065,   224,  3412, 12843,   191,  3324,\n",
      "        12831,  8770, 12857,  3796, 13581,   221,  1740,   223,  1020, 14291,\n",
      "         9356,   768, 14303, 14168,  8878,   219,  1979,  3786,  3152,  9659,\n",
      "         6856,   221,  1478, 11615, 12761, 14291,   226, 10429, 12697,   223,\n",
      "         8243,  8986,  3786, 14291,  7567,  7607,  8243,   224,   618,   219,\n",
      "        11807,  8243, 11569,  9372, 10654,   219,  1478, 14065, 13658,  2722,\n",
      "        12221,   202,  3061,  8684,  9271,   221,  1152,   709,   223,  2930,\n",
      "          219,  1478,   162,  7604,  8243,   219, 11835,   223, 14278, 14291,\n",
      "         9356,  9545, 12873, 13010,  6834,  9660,  9761,  5813, 14121,  3061,\n",
      "        12697,   223,  4389,   219,  3400,   202, 10224, 14291,   219,  5582,\n",
      "          221,  1740,   223,  2665, 13840,  3061, 12986, 13981,  3324, 12831,\n",
      "         4132,   191,  9045,  2214,   130,  3277, 12831,  3916,   219, 12963,\n",
      "         3152,  8243,   223,  2660,  9704,  8636,  3061,  7439,  8243,  5721,\n",
      "        10938,  1478,     3, 12831,  9101,  9583,     3, 12831,  4132,   219,\n",
      "         7932,  3412, 13293,   219,  2556,  5356, 12831, 13681,   219,  9460,\n",
      "         3804,  1693,  8414, 14121, 12831, 11064,   219, 13984, 12831,  9756,\n",
      "         8161,   919, 11488,  3412,  7604,   219,  5698,   219,  8155,   219,\n",
      "        13793,   219,  6626,   219,   350,   219,  9370,  9926,   219,  5721,\n",
      "         9139,  2833, 12831,  3473,  3412,  3249,  4899,  1958, 12831, 14028,\n",
      "         4132,   221,  2661,  3916,  3430,   130,   220,  1152,   709,   223])\n"
     ]
    }
   ],
   "source": [
    "# encode the entire text dataset and store it into a torch.Tensor\n",
    "data = torch.tensor(encode(word_tokenize(text)), dtype=torch.long)\n",
    "print(data.shape, data.type)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of train data is : 229058\n",
      "The size of val data is : 25451\n"
     ]
    }
   ],
   "source": [
    "# Let's now split up the data into train and validation sets\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(f'The size of train data is : {len(train_data)}')\n",
    "print(f'The size of val data is : {len(val_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1152,   709,   223,   482, 13877, 10480,  3440,  7080,   219])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: tensor([1152]), target: 709\n",
      "input: tensor([1152,  709]), target: 223\n",
      "input: tensor([1152,  709,  223]), target: 482\n",
      "input: tensor([1152,  709,  223,  482]), target: 13877\n",
      "input: tensor([ 1152,   709,   223,   482, 13877]), target: 10480\n",
      "input: tensor([ 1152,   709,   223,   482, 13877, 10480]), target: 3440\n",
      "input: tensor([ 1152,   709,   223,   482, 13877, 10480,  3440]), target: 7080\n",
      "input: tensor([ 1152,   709,   223,   482, 13877, 10480,  3440,  7080]), target: 219\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f'input: {context}, target: {target}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # How many independent sequences will be process in parallel?\n",
    "block_size = 8 # What is the maximum context length for predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unseprated batch:\n",
      "tensor([ 1152,   709,   223,   482, 13877, 10480,  3440,  7080,   219,  7604,\n",
      "         8993, 12087,   221,   323,   223,  2520,   219, 12087,   221,  1152,\n",
      "          709,   223,  3053,  3512,  3324, 11053, 10791, 13010,  5723, 12819,\n",
      "        13010,  6533]) \n",
      "\n",
      "Sperated batch:\n",
      "tensor([[ 1152,   709,   223,   482, 13877, 10480,  3440,  7080],\n",
      "        [  219,  7604,  8993, 12087,   221,   323,   223,  2520],\n",
      "        [  219, 12087,   221,  1152,   709,   223,  3053,  3512],\n",
      "        [ 3324, 11053, 10791, 13010,  5723, 12819, 13010,  6533]])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "sindex = 0\n",
    "eindex = batch_size * block_size\n",
    "batch = data[sindex: eindex]\n",
    "print(\"Unseprated batch:\")\n",
    "print(batch, \"\\n\")\n",
    "batch = batch.reshape((batch_size, block_size))\n",
    "print(f\"Sperated batch:\")\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch_size, split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    \n",
    "    sindex = 0\n",
    "    eindex = batch_size * block_size\n",
    "    while eindex < (len(data) - block_size):\n",
    "        # Extract batch_size * block_size tokens in the data\n",
    "        xb = data[sindex: eindex]\n",
    "        yb = data[sindex+1: eindex+1]\n",
    "        # Reshape the batch by the shape (batch_size, block_size)\n",
    "        xb = xb.reshape(batch_size, block_size)\n",
    "        yb = yb.reshape(batch_size, block_size)\n",
    "        # Update the indexes for extracting tokens sections\n",
    "        temp = eindex\n",
    "        eindex = eindex + block_size * batch_size\n",
    "        sindex = temp\n",
    "        yield xb, yb\n",
    "    \n",
    "    # For the last batch that cannot be batched as in the size (batch_size, block_size)\n",
    "    # Maintain the 'block_size' dimension\n",
    "    if eindex >= (len(data) - block_size):\n",
    "        # The number of useable completed sample including 'block_size' elements\n",
    "        num_sample = (len(data) - sindex) // block_size \n",
    "        if num_sample >= 1:\n",
    "            xb = data[sindex: sindex + (num_sample * block_size)]\n",
    "            yb = data[sindex+1: sindex + (num_sample * block_size)+1]\n",
    "            xb = xb.reshape(num_sample, block_size)\n",
    "            yb = yb.reshape(num_sample, block_size)\n",
    "            yield xb, yb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([32, 32])\n",
      "tensor([[ 1152,   709,   223,  ..., 12819, 13010,  6533],\n",
      "        [  225,   323,   223,  ...,  8404,   221,  1152],\n",
      "        [  709,   223,  1679,  ...,  8580,  8243,  3786],\n",
      "        ...,\n",
      "        [ 8243,  5721, 10938,  ..., 13984, 12831,  9756],\n",
      "        [ 8161,   919, 11488,  ..., 14028,  4132,   221],\n",
      "        [ 2661,  3916,  3430,  ...,  8348,  9659, 11942]])\n",
      "targets:\n",
      "torch.Size([32, 32])\n",
      "tensor([[  709,   223,   482,  ..., 13010,  6533,   225],\n",
      "        [  323,   223,  2256,  ...,   221,  1152,   709],\n",
      "        [  223,  1679, 13581,  ...,  8243,  3786,  5952],\n",
      "        ...,\n",
      "        [ 5721, 10938,  1478,  ..., 12831,  9756,  8161],\n",
      "        [  919, 11488,  3412,  ...,  4132,   221,  2661],\n",
      "        [ 3916,  3430,   130,  ...,  9659, 11942,   219]])\n",
      "TOTAL number of batches: 224\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "block_size = 32\n",
    "\n",
    "num_batch = 0\n",
    "for i, batch in enumerate(get_batch(batch_size, 'train')):\n",
    "    xb, yb = batch\n",
    "    if i < 1:\n",
    "        print('inputs:')\n",
    "        print(xb.shape)\n",
    "        print(xb)\n",
    "        print('targets:')\n",
    "        print(yb.shape)\n",
    "        print(yb)\n",
    "    num_batch += 1\n",
    "\n",
    "print(f'TOTAL number of batches: {num_batch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1/4\n",
      "when input is [11835] the target: 219\n",
      "when input is [11835, 219] the target: 9085\n",
      "when input is [11835, 219, 9085] the target: 14291\n",
      "when input is [11835, 219, 9085, 14291] the target: 13793\n",
      "when input is [11835, 219, 9085, 14291, 13793] the target: 8636\n",
      "when input is [11835, 219, 9085, 14291, 13793, 8636] the target: 3061\n",
      "when input is [11835, 219, 9085, 14291, 13793, 8636, 3061] the target: 12377\n",
      "when input is [11835, 219, 9085, 14291, 13793, 8636, 3061, 12377] the target: 223\n",
      "\n",
      "batch 2/4\n",
      "when input is [223] the target: 8986\n",
      "when input is [223, 8986] the target: 1478\n",
      "when input is [223, 8986, 1478] the target: 3786\n",
      "when input is [223, 8986, 1478, 3786] the target: 11970\n",
      "when input is [223, 8986, 1478, 3786, 11970] the target: 4137\n",
      "when input is [223, 8986, 1478, 3786, 11970, 4137] the target: 13010\n",
      "when input is [223, 8986, 1478, 3786, 11970, 4137, 13010] the target: 8402\n",
      "when input is [223, 8986, 1478, 3786, 11970, 4137, 13010, 8402] the target: 12831\n",
      "\n",
      "batch 3/4\n",
      "when input is [12831] the target: 4547\n",
      "when input is [12831, 4547] the target: 9659\n",
      "when input is [12831, 4547, 9659] the target: 14300\n",
      "when input is [12831, 4547, 9659, 14300] the target: 4878\n",
      "when input is [12831, 4547, 9659, 14300, 4878] the target: 225\n",
      "when input is [12831, 4547, 9659, 14300, 4878, 225] the target: 2616\n",
      "when input is [12831, 4547, 9659, 14300, 4878, 225, 2616] the target: 223\n",
      "when input is [12831, 4547, 9659, 14300, 4878, 225, 2616, 223] the target: 2010\n",
      "\n",
      "batch 4/4\n",
      "when input is [2010] the target: 8993\n",
      "when input is [2010, 8993] the target: 219\n",
      "when input is [2010, 8993, 219] the target: 11835\n",
      "when input is [2010, 8993, 219, 11835] the target: 219\n",
      "when input is [2010, 8993, 219, 11835, 219] the target: 12831\n",
      "when input is [2010, 8993, 219, 11835, 219, 12831] the target: 4141\n",
      "when input is [2010, 8993, 219, 11835, 219, 12831, 4141] the target: 8232\n",
      "when input is [2010, 8993, 219, 11835, 219, 12831, 4141, 8232] the target: 9132\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for b in range(batch_size): # batch dimension\n",
    "    print(f'batch {b+1}/{batch_size}')\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
