{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/names.txt', 'r') as f:\n",
    "    names_list = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all characters in the name_list\n",
    "all_chars = sorted(list(set(''.join(names_list))))\n",
    "\n",
    "# Encoding alphabet using numbering \n",
    "# Also using the char '.' to replace the <S> and <E>, the '.' is denoted as 0\n",
    "stoi = {s:i+1 for i, s in enumerate(all_chars)}\n",
    "stoi['.'] = 0\n",
    "\n",
    "# Decoding\n",
    "itos = {i:s for s, i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def built_dataset(text):\n",
    "    xs, ys = [], []\n",
    "\n",
    "    for t in text:\n",
    "        t_str = ['.'] + list(t) + ['.']\n",
    "        for c1, c2 in zip(t_str, t_str[1:]):\n",
    "            xs.append(stoi[c1])\n",
    "            ys.append(stoi[c2])\n",
    "\n",
    "    return torch.tensor(xs), torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = built_dataset(text=names_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the xs by one-hot encoding\n",
    "xs_encoded = F.one_hot(xs, num_classes=27).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The generator\n",
    "g = torch.Generator().manual_seed(283839281)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The weight matrix\n",
    "W = torch.randn(size=(27, 27), generator=g, requires_grad=True) # The weight matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, lr):\n",
    "    for k in range(epochs):\n",
    "        # forward pass\n",
    "        logits = xs_encoded @ W                     \n",
    "        counts = logits.exp()\n",
    "        probs = counts / counts.sum(1, keepdims=True)\n",
    "\n",
    "        # The loss function\n",
    "        loss = -probs[torch.arange(xs.nelement()), ys].log().mean() \n",
    "        if k % 10 == 0:\n",
    "            print(\"epoch {}: loss = {}\".format(k, loss))\n",
    "\n",
    "        # Backpropagation\n",
    "        W.grad = None # Set the gradient to zero (using None here)\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the network by the gradient\n",
    "        W.data += -lr * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: loss = 3.8166439533233643\n",
      "epoch 10: loss = 3.428638458251953\n",
      "epoch 20: loss = 3.190485954284668\n",
      "epoch 30: loss = 3.0372958183288574\n",
      "epoch 40: loss = 2.9339752197265625\n",
      "epoch 50: loss = 2.8602824211120605\n",
      "epoch 60: loss = 2.804910659790039\n",
      "epoch 70: loss = 2.761800765991211\n",
      "epoch 80: loss = 2.7273361682891846\n",
      "epoch 90: loss = 2.6991896629333496\n",
      "epoch 100: loss = 2.675816059112549\n",
      "epoch 110: loss = 2.6561551094055176\n",
      "epoch 120: loss = 2.639446258544922\n",
      "epoch 130: loss = 2.6251208782196045\n",
      "epoch 140: loss = 2.612736940383911\n",
      "epoch 150: loss = 2.601944923400879\n",
      "epoch 160: loss = 2.5924651622772217\n",
      "epoch 170: loss = 2.584073543548584\n",
      "epoch 180: loss = 2.5765907764434814\n",
      "epoch 190: loss = 2.569873332977295\n",
      "epoch 200: loss = 2.56380558013916\n",
      "epoch 210: loss = 2.558295488357544\n",
      "epoch 220: loss = 2.55326771736145\n",
      "epoch 230: loss = 2.548661231994629\n",
      "epoch 240: loss = 2.5444252490997314\n",
      "epoch 250: loss = 2.5405187606811523\n",
      "epoch 260: loss = 2.5369060039520264\n",
      "epoch 270: loss = 2.533557176589966\n",
      "epoch 280: loss = 2.530447006225586\n",
      "epoch 290: loss = 2.527553081512451\n",
      "epoch 300: loss = 2.5248563289642334\n",
      "epoch 310: loss = 2.5223400592803955\n",
      "epoch 320: loss = 2.5199882984161377\n",
      "epoch 330: loss = 2.5177876949310303\n",
      "epoch 340: loss = 2.515725612640381\n",
      "epoch 350: loss = 2.5137908458709717\n",
      "epoch 360: loss = 2.5119729042053223\n",
      "epoch 370: loss = 2.5102622509002686\n",
      "epoch 380: loss = 2.508650779724121\n",
      "epoch 390: loss = 2.507129430770874\n",
      "epoch 400: loss = 2.5056915283203125\n",
      "epoch 410: loss = 2.504330635070801\n",
      "epoch 420: loss = 2.503040313720703\n",
      "epoch 430: loss = 2.5018150806427\n",
      "epoch 440: loss = 2.5006496906280518\n",
      "epoch 450: loss = 2.499540090560913\n",
      "epoch 460: loss = 2.4984819889068604\n",
      "epoch 470: loss = 2.497471570968628\n",
      "epoch 480: loss = 2.4965052604675293\n",
      "epoch 490: loss = 2.4955801963806152\n"
     ]
    }
   ],
   "source": [
    "train(epochs=500, lr=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deri\n",
      "kay\n",
      "hieprbchahy\n",
      "wran\n",
      "iyah\n",
      "mo\n",
      "e\n",
      "men\n",
      "canty\n",
      "ph\n",
      "aya\n",
      "ery\n",
      "aghafiauiylae\n",
      "st\n",
      "a\n",
      "bea\n",
      "zzm\n",
      "y\n",
      "brvelans\n",
      "kos\n"
     ]
    }
   ],
   "source": [
    "gg = torch.Generator().manual_seed(23879322)\n",
    "\n",
    "for i in range(20):\n",
    "    ix = 0 # The start token of the generated name\n",
    "    generated_name = []\n",
    "    while True:\n",
    "        xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n",
    "        logits = xenc @ W\n",
    "        counts = logits.exp()\n",
    "        p = counts / counts.sum(1, keepdims=True)\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=gg).item()\n",
    "        if ix != 0: # If not reach the end token, then keep appending all chars\n",
    "            generated_name.append(itos[ix])\n",
    "        else: # The end token of the generated name\n",
    "            break\n",
    "\n",
    "    print(''.join(generated_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e76b06f811d914f25ddf1d876c9e6424e54248baadb52cf54ff8d72e027625bb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
