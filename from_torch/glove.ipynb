{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An implementation of GloVe (Global vectors for word representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "from itertools import combinations_with_replacement\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x215a1c03710>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(2345678)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data using the NLTK corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.gutenberg.fileids() # Show all texts in the Gutenberg corpus in the NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the word to index mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['piggledy', 'worm', 'have', 'this', 'greek']\n",
      "Total words in the text: 258\n"
     ]
    }
   ],
   "source": [
    "# Select the melville-moby_dick.txt as the training sample\n",
    "melville_words = nltk.corpus.gutenberg.words('melville-moby_dick.txt')[:500]\n",
    "\n",
    "# Lower all words\n",
    "vocab = list(set([w.lower() for w in melville_words]))\n",
    "\n",
    "# Show the first five words in the processed list \n",
    "print(vocab[:5])\n",
    "print(f'Total words in the text: {len(vocab)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build co-occurence blobs (the $X_{ij}$ values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a word co-occurrence list for the given corpus, where each element (representing\n",
    "a cooccurrence pair) is of the form $(i_{id}, j_{id}, X_{ij})$, where \n",
    "$i_{id}$   : the ID of the main word in the cooccurrence;\n",
    "$j_{id}$: the ID of the context word;\n",
    "$X_{ij}$: the the cooccurrence value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'moby', 'dick', 'by', 'herman', 'melville', '1851', ']']\n",
      "['etymology', '.']\n",
      "['(', 'supplied', 'by', 'a', 'late', 'consumptive', 'usher', 'to', 'a', 'grammar', 'school', ')']\n",
      "['the', 'pale', 'usher', '--', 'threadbare', 'in', 'coat', ',', 'heart', ',', 'body', ',', 'and', 'brain', ';', 'i', 'see', 'him', 'now', '.']\n",
      "['he', 'was', 'ever', 'dusting', 'his', 'old', 'lexicons', 'and', 'grammars', ',', 'with', 'a', 'queer', 'handkerchief', ',', 'mockingly', 'embellished', 'with', 'all', 'the', 'gay', 'flags', 'of', 'all', 'the', 'known', 'nations', 'of', 'the', 'world', '.']\n"
     ]
    }
   ],
   "source": [
    "melville_sents = nltk.corpus.gutenberg.sents('melville-moby_dick.txt')[:500]\n",
    "melville_sents = [[w.lower() for w in s] for s in melville_sents]\n",
    "\n",
    "for sent in melville_sents[:5]:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "vocab = list(set(flatten(melville_sents)))\n",
    "\n",
    "# Mark each word in the vocab list by its index\n",
    "word2index = {word:index for index, word in enumerate(vocab)}\n",
    "# print(word2index)\n",
    "\n",
    "# Also build the integer to word list\n",
    "index2word = {index:word for word, index in word2index.items()}\n",
    "# print(index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<S>', '<S>', '<S>', '<S>', '<S>', '[', 'moby', 'dick', 'by', 'herman', 'melville', '1851', ']', '<E>', '<E>', '<E>', '<E>', '<E>']\n",
      "['<S>', '<S>', '<S>', '<S>', '<S>', 'etymology', '.', '<E>', '<E>', '<E>', '<E>', '<E>']\n",
      "['<S>', '<S>', '<S>', '<S>', '<S>', '(', 'supplied', 'by', 'a', 'late', 'consumptive', 'usher', 'to', 'a', 'grammar', 'school', ')', '<E>', '<E>', '<E>', '<E>', '<E>']\n",
      "['<S>', '<S>', '<S>', '<S>', '<S>', 'the', 'pale', 'usher', '--', 'threadbare', 'in', 'coat', ',', 'heart', ',', 'body', ',', 'and', 'brain', ';', 'i', 'see', 'him', 'now', '.', '<E>', '<E>', '<E>', '<E>', '<E>']\n",
      "['<S>', '<S>', '<S>', '<S>', '<S>', 'he', 'was', 'ever', 'dusting', 'his', 'old', 'lexicons', 'and', 'grammars', ',', 'with', 'a', 'queer', 'handkerchief', ',', 'mockingly', 'embellished', 'with', 'all', 'the', 'gay', 'flags', 'of', 'all', 'the', 'known', 'nations', 'of', 'the', 'world', '.', '<E>', '<E>', '<E>', '<E>', '<E>']\n"
     ]
    }
   ],
   "source": [
    "# Insert each sentence with <S> at the begining and append <E> at the end\n",
    "WINDOW_SIZE = 5\n",
    "_sents = [['<S>'] * WINDOW_SIZE + s + ['<E>'] * WINDOW_SIZE for s in melville_sents]\n",
    "\n",
    "for s in _sents[:5]:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<S>', '<S>', '<S>', '<S>', '<S>', '[', 'moby', 'dick', 'by', 'herman', 'melville')\n",
      "('<S>', '<S>', '<S>', '<S>', '[', 'moby', 'dick', 'by', 'herman', 'melville', '1851')\n",
      "('<S>', '<S>', '<S>', '[', 'moby', 'dick', 'by', 'herman', 'melville', '1851', ']')\n",
      "('<S>', '<S>', '[', 'moby', 'dick', 'by', 'herman', 'melville', '1851', ']', '<E>')\n",
      "('<S>', '[', 'moby', 'dick', 'by', 'herman', 'melville', '1851', ']', '<E>', '<E>')\n",
      "('[', 'moby', 'dick', 'by', 'herman', 'melville', '1851', ']', '<E>', '<E>', '<E>')\n",
      "('moby', 'dick', 'by', 'herman', 'melville', '1851', ']', '<E>', '<E>', '<E>', '<E>')\n",
      "('dick', 'by', 'herman', 'melville', '1851', ']', '<E>', '<E>', '<E>', '<E>', '<E>')\n",
      "('<S>', '<S>', '<S>', '<S>', '<S>', 'etymology', '.', '<E>', '<E>', '<E>', '<E>')\n",
      "('<S>', '<S>', '<S>', '<S>', 'etymology', '.', '<E>', '<E>', '<E>', '<E>', '<E>')\n",
      "('<S>', '<S>', '<S>', '<S>', '<S>', '(', 'supplied', 'by', 'a', 'late', 'consumptive')\n",
      "('<S>', '<S>', '<S>', '<S>', '(', 'supplied', 'by', 'a', 'late', 'consumptive', 'usher')\n",
      "('<S>', '<S>', '<S>', '(', 'supplied', 'by', 'a', 'late', 'consumptive', 'usher', 'to')\n",
      "('<S>', '<S>', '(', 'supplied', 'by', 'a', 'late', 'consumptive', 'usher', 'to', 'a')\n",
      "('<S>', '(', 'supplied', 'by', 'a', 'late', 'consumptive', 'usher', 'to', 'a', 'grammar')\n",
      "('(', 'supplied', 'by', 'a', 'late', 'consumptive', 'usher', 'to', 'a', 'grammar', 'school')\n",
      "('supplied', 'by', 'a', 'late', 'consumptive', 'usher', 'to', 'a', 'grammar', 'school', ')')\n",
      "('by', 'a', 'late', 'consumptive', 'usher', 'to', 'a', 'grammar', 'school', ')', '<E>')\n",
      "('a', 'late', 'consumptive', 'usher', 'to', 'a', 'grammar', 'school', ')', '<E>', '<E>')\n",
      "('late', 'consumptive', 'usher', 'to', 'a', 'grammar', 'school', ')', '<E>', '<E>', '<E>')\n"
     ]
    }
   ],
   "source": [
    "# Construct the ngrams for building word pairs (i, j) \n",
    "windows = flatten([list(nltk.ngrams(s, WINDOW_SIZE * 2 + 1)) for s in _sents])\n",
    "\n",
    "for s in windows[:20]:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building word pairs\n",
    "word_pairs = []\n",
    "\n",
    "for window in windows:\n",
    "    for i in range(WINDOW_SIZE * 2 + 1):\n",
    "        # Center at the word at index WINDOW_SIZE (which is at index 5), denoted as i\n",
    "        # Ignore <S> and <E> and the word i itself, all other words in the windows are context, denoted as j \n",
    "        # Then make the pair in the form as (i, j)\n",
    "        if i == WINDOW_SIZE or window[i] == '<S>' or window[i] == '<E>': \n",
    "            continue\n",
    "        word = window[WINDOW_SIZE]\n",
    "        context = window[i]\n",
    "        word_pairs.append((word, context))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[', 'moby'), ('[', 'dick'), ('[', 'by'), ('[', 'herman'), ('[', 'melville'), ('moby', '['), ('moby', 'dick'), ('moby', 'by'), ('moby', 'herman'), ('moby', 'melville')]\n"
     ]
    }
   ],
   "source": [
    "print(word_pairs[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build co-occurence matrix $X$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record the occurence time of each word in the corpus, denoted as X_i\n",
    "X_i = Counter(flatten(melville_sents))\n",
    "\n",
    "# Record the co-occurence time of each word within window_size using onstructed word pairs \n",
    "X_ik_window = Counter(word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('[', 'moby'): 1,\n",
       "         ('[', 'dick'): 1,\n",
       "         ('[', 'by'): 1,\n",
       "         ('[', 'herman'): 1,\n",
       "         ('[', 'melville'): 1,\n",
       "         ('moby', '['): 1,\n",
       "         ('moby', 'dick'): 1,\n",
       "         ('moby', 'by'): 1,\n",
       "         ('moby', 'herman'): 1,\n",
       "         ('moby', 'melville'): 1,\n",
       "         ('moby', '1851'): 1,\n",
       "         ('dick', '['): 1,\n",
       "         ('dick', 'moby'): 1,\n",
       "         ('dick', 'by'): 1,\n",
       "         ('dick', 'herman'): 1,\n",
       "         ('dick', 'melville'): 1,\n",
       "         ('dick', '1851'): 1,\n",
       "         ('dick', ']'): 1,\n",
       "         ('by', '['): 1,\n",
       "         ('by', 'moby'): 1,\n",
       "         ('by', 'dick'): 1,\n",
       "         ('by', 'herman'): 1,\n",
       "         ('by', 'melville'): 1,\n",
       "         ('by', '1851'): 1,\n",
       "         ('by', ']'): 1,\n",
       "         ('herman', '['): 1,\n",
       "         ('herman', 'moby'): 1,\n",
       "         ('herman', 'dick'): 1,\n",
       "         ('herman', 'by'): 1,\n",
       "         ('herman', 'melville'): 1,\n",
       "         ('herman', '1851'): 1,\n",
       "         ('herman', ']'): 1,\n",
       "         ('melville', '['): 1,\n",
       "         ('melville', 'moby'): 1,\n",
       "         ('melville', 'dick'): 1,\n",
       "         ('melville', 'by'): 1,\n",
       "         ('melville', 'herman'): 1,\n",
       "         ('melville', '1851'): 1,\n",
       "         ('melville', ']'): 1,\n",
       "         ('1851', 'moby'): 1,\n",
       "         ('1851', 'dick'): 1,\n",
       "         ('1851', 'by'): 1,\n",
       "         ('1851', 'herman'): 1,\n",
       "         ('1851', 'melville'): 1,\n",
       "         ('1851', ']'): 1,\n",
       "         (']', 'dick'): 1,\n",
       "         (']', 'by'): 1,\n",
       "         (']', 'herman'): 1,\n",
       "         (']', 'melville'): 1,\n",
       "         (']', '1851'): 1,\n",
       "         ('etymology', '.'): 1,\n",
       "         ('.', 'etymology'): 1,\n",
       "         ('(', 'supplied'): 2,\n",
       "         ('(', 'by'): 4,\n",
       "         ('(', 'a'): 3,\n",
       "         ('(', 'late'): 1,\n",
       "         ('(', 'consumptive'): 1,\n",
       "         ('supplied', '('): 2,\n",
       "         ('supplied', 'by'): 2,\n",
       "         ('supplied', 'a'): 3,\n",
       "         ('supplied', 'late'): 1,\n",
       "         ('supplied', 'consumptive'): 1,\n",
       "         ('supplied', 'usher'): 1,\n",
       "         ('by', '('): 4,\n",
       "         ('by', 'supplied'): 2,\n",
       "         ('by', 'a'): 19,\n",
       "         ('by', 'late'): 1,\n",
       "         ('by', 'consumptive'): 1,\n",
       "         ('by', 'usher'): 1,\n",
       "         ('by', 'to'): 7,\n",
       "         ('a', '('): 3,\n",
       "         ('a', 'supplied'): 3,\n",
       "         ('a', 'by'): 19,\n",
       "         ('a', 'late'): 2,\n",
       "         ('a', 'consumptive'): 2,\n",
       "         ('a', 'usher'): 2,\n",
       "         ('a', 'to'): 57,\n",
       "         ('a', 'a'): 48,\n",
       "         ('late', '('): 1,\n",
       "         ('late', 'supplied'): 1,\n",
       "         ('late', 'by'): 1,\n",
       "         ('late', 'a'): 2,\n",
       "         ('late', 'consumptive'): 1,\n",
       "         ('late', 'usher'): 1,\n",
       "         ('late', 'to'): 2,\n",
       "         ('late', 'grammar'): 1,\n",
       "         ('consumptive', '('): 1,\n",
       "         ('consumptive', 'supplied'): 1,\n",
       "         ('consumptive', 'by'): 1,\n",
       "         ('consumptive', 'a'): 2,\n",
       "         ('consumptive', 'late'): 1,\n",
       "         ('consumptive', 'usher'): 1,\n",
       "         ('consumptive', 'to'): 1,\n",
       "         ('consumptive', 'grammar'): 1,\n",
       "         ('consumptive', 'school'): 1,\n",
       "         ('usher', 'supplied'): 1,\n",
       "         ('usher', 'by'): 1,\n",
       "         ('usher', 'a'): 2,\n",
       "         ('usher', 'late'): 1,\n",
       "         ('usher', 'consumptive'): 1,\n",
       "         ('usher', 'to'): 1,\n",
       "         ('usher', 'grammar'): 1,\n",
       "         ('usher', 'school'): 1,\n",
       "         ('usher', ')'): 1,\n",
       "         ('to', 'by'): 7,\n",
       "         ('to', 'a'): 57,\n",
       "         ('to', 'late'): 2,\n",
       "         ('to', 'consumptive'): 1,\n",
       "         ('to', 'usher'): 1,\n",
       "         ('to', 'grammar'): 1,\n",
       "         ('to', 'school'): 3,\n",
       "         ('to', ')'): 1,\n",
       "         ('a', 'grammar'): 1,\n",
       "         ('a', 'school'): 1,\n",
       "         ('a', ')'): 3,\n",
       "         ('grammar', 'late'): 1,\n",
       "         ('grammar', 'consumptive'): 1,\n",
       "         ('grammar', 'usher'): 1,\n",
       "         ('grammar', 'to'): 1,\n",
       "         ('grammar', 'a'): 1,\n",
       "         ('grammar', 'school'): 1,\n",
       "         ('grammar', ')'): 1,\n",
       "         ('school', 'consumptive'): 1,\n",
       "         ('school', 'usher'): 1,\n",
       "         ('school', 'to'): 3,\n",
       "         ('school', 'a'): 1,\n",
       "         ('school', 'grammar'): 1,\n",
       "         ('school', ')'): 1,\n",
       "         (')', 'usher'): 1,\n",
       "         (')', 'to'): 1,\n",
       "         (')', 'a'): 3,\n",
       "         (')', 'grammar'): 1,\n",
       "         (')', 'school'): 1,\n",
       "         ('the', 'pale'): 3,\n",
       "         ('the', 'usher'): 1,\n",
       "         ('the', '--'): 23,\n",
       "         ('the', 'threadbare'): 1,\n",
       "         ('the', 'in'): 113,\n",
       "         ('pale', 'the'): 3,\n",
       "         ('pale', 'usher'): 1,\n",
       "         ('pale', '--'): 1,\n",
       "         ('pale', 'threadbare'): 1,\n",
       "         ('pale', 'in'): 1,\n",
       "         ('pale', 'coat'): 1,\n",
       "         ('usher', 'the'): 1,\n",
       "         ('usher', 'pale'): 1,\n",
       "         ('usher', '--'): 1,\n",
       "         ('usher', 'threadbare'): 1,\n",
       "         ('usher', 'in'): 1,\n",
       "         ('usher', 'coat'): 1,\n",
       "         ('usher', ','): 1,\n",
       "         ('--', 'the'): 23,\n",
       "         ('--', 'pale'): 1,\n",
       "         ('--', 'usher'): 1,\n",
       "         ('--', 'threadbare'): 1,\n",
       "         ('--', 'in'): 13,\n",
       "         ('--', 'coat'): 1,\n",
       "         ('--', ','): 17,\n",
       "         ('--', 'heart'): 1,\n",
       "         ('threadbare', 'the'): 1,\n",
       "         ('threadbare', 'pale'): 1,\n",
       "         ('threadbare', 'usher'): 1,\n",
       "         ('threadbare', '--'): 1,\n",
       "         ('threadbare', 'in'): 1,\n",
       "         ('threadbare', 'coat'): 1,\n",
       "         ('threadbare', ','): 2,\n",
       "         ('threadbare', 'heart'): 1,\n",
       "         ('in', 'the'): 113,\n",
       "         ('in', 'pale'): 1,\n",
       "         ('in', 'usher'): 1,\n",
       "         ('in', '--'): 13,\n",
       "         ('in', 'threadbare'): 1,\n",
       "         ('in', 'coat'): 1,\n",
       "         ('in', ','): 125,\n",
       "         ('in', 'heart'): 1,\n",
       "         ('in', 'body'): 2,\n",
       "         ('coat', 'pale'): 1,\n",
       "         ('coat', 'usher'): 1,\n",
       "         ('coat', '--'): 1,\n",
       "         ('coat', 'threadbare'): 1,\n",
       "         ('coat', 'in'): 1,\n",
       "         ('coat', ','): 4,\n",
       "         ('coat', 'heart'): 1,\n",
       "         ('coat', 'body'): 1,\n",
       "         (',', 'usher'): 1,\n",
       "         (',', '--'): 17,\n",
       "         (',', 'threadbare'): 2,\n",
       "         (',', 'in'): 125,\n",
       "         (',', 'coat'): 4,\n",
       "         (',', 'heart'): 4,\n",
       "         (',', ','): 342,\n",
       "         (',', 'body'): 6,\n",
       "         (',', 'and'): 229,\n",
       "         ('heart', '--'): 1,\n",
       "         ('heart', 'threadbare'): 1,\n",
       "         ('heart', 'in'): 1,\n",
       "         ('heart', 'coat'): 1,\n",
       "         ('heart', ','): 4,\n",
       "         ('heart', 'body'): 1,\n",
       "         ('heart', 'and'): 1,\n",
       "         ('heart', 'brain'): 1,\n",
       "         (',', 'brain'): 2,\n",
       "         (',', ';'): 25,\n",
       "         ('body', 'in'): 2,\n",
       "         ('body', 'coat'): 1,\n",
       "         ('body', ','): 6,\n",
       "         ('body', 'heart'): 1,\n",
       "         ('body', 'and'): 2,\n",
       "         ('body', 'brain'): 1,\n",
       "         ('body', ';'): 1,\n",
       "         ('body', 'i'): 1,\n",
       "         (',', 'i'): 74,\n",
       "         (',', 'see'): 6,\n",
       "         ('and', ','): 229,\n",
       "         ('and', 'heart'): 1,\n",
       "         ('and', 'body'): 2,\n",
       "         ('and', 'brain'): 1,\n",
       "         ('and', ';'): 26,\n",
       "         ('and', 'i'): 13,\n",
       "         ('and', 'see'): 4,\n",
       "         ('and', 'him'): 6,\n",
       "         ('brain', 'heart'): 1,\n",
       "         ('brain', ','): 2,\n",
       "         ('brain', 'body'): 1,\n",
       "         ('brain', 'and'): 1,\n",
       "         ('brain', ';'): 1,\n",
       "         ('brain', 'i'): 1,\n",
       "         ('brain', 'see'): 1,\n",
       "         ('brain', 'him'): 1,\n",
       "         ('brain', 'now'): 1,\n",
       "         (';', ','): 25,\n",
       "         (';', 'body'): 1,\n",
       "         (';', 'and'): 26,\n",
       "         (';', 'brain'): 1,\n",
       "         (';', 'i'): 10,\n",
       "         (';', 'see'): 1,\n",
       "         (';', 'him'): 5,\n",
       "         (';', 'now'): 5,\n",
       "         (';', '.'): 6,\n",
       "         ('i', 'body'): 1,\n",
       "         ('i', ','): 74,\n",
       "         ('i', 'and'): 13,\n",
       "         ('i', 'brain'): 1,\n",
       "         ('i', ';'): 10,\n",
       "         ('i', 'see'): 3,\n",
       "         ('i', 'him'): 4,\n",
       "         ('i', 'now'): 5,\n",
       "         ('i', '.'): 9,\n",
       "         ('see', ','): 6,\n",
       "         ('see', 'and'): 4,\n",
       "         ('see', 'brain'): 1,\n",
       "         ('see', ';'): 1,\n",
       "         ('see', 'i'): 3,\n",
       "         ('see', 'him'): 1,\n",
       "         ('see', 'now'): 2,\n",
       "         ('see', '.'): 1,\n",
       "         ('him', 'and'): 6,\n",
       "         ('him', 'brain'): 1,\n",
       "         ('him', ';'): 5,\n",
       "         ('him', 'i'): 4,\n",
       "         ('him', 'see'): 1,\n",
       "         ('him', 'now'): 1,\n",
       "         ('him', '.'): 3,\n",
       "         ('now', 'brain'): 1,\n",
       "         ('now', ';'): 5,\n",
       "         ('now', 'i'): 5,\n",
       "         ('now', 'see'): 2,\n",
       "         ('now', 'him'): 1,\n",
       "         ('now', '.'): 2,\n",
       "         ('.', ';'): 6,\n",
       "         ('.', 'i'): 9,\n",
       "         ('.', 'see'): 1,\n",
       "         ('.', 'him'): 3,\n",
       "         ('.', 'now'): 2,\n",
       "         ('he', 'was'): 3,\n",
       "         ('he', 'ever'): 1,\n",
       "         ('he', 'dusting'): 1,\n",
       "         ('he', 'his'): 15,\n",
       "         ('he', 'old'): 2,\n",
       "         ('was', 'he'): 3,\n",
       "         ('was', 'ever'): 1,\n",
       "         ('was', 'dusting'): 1,\n",
       "         ('was', 'his'): 2,\n",
       "         ('was', 'old'): 1,\n",
       "         ('was', 'lexicons'): 1,\n",
       "         ('ever', 'he'): 1,\n",
       "         ('ever', 'was'): 1,\n",
       "         ('ever', 'dusting'): 1,\n",
       "         ('ever', 'his'): 1,\n",
       "         ('ever', 'old'): 1,\n",
       "         ('ever', 'lexicons'): 1,\n",
       "         ('ever', 'and'): 5,\n",
       "         ('dusting', 'he'): 1,\n",
       "         ('dusting', 'was'): 1,\n",
       "         ('dusting', 'ever'): 1,\n",
       "         ('dusting', 'his'): 1,\n",
       "         ('dusting', 'old'): 1,\n",
       "         ('dusting', 'lexicons'): 1,\n",
       "         ('dusting', 'and'): 1,\n",
       "         ('dusting', 'grammars'): 1,\n",
       "         ('his', 'he'): 15,\n",
       "         ('his', 'was'): 2,\n",
       "         ('his', 'ever'): 1,\n",
       "         ('his', 'dusting'): 1,\n",
       "         ('his', 'old'): 3,\n",
       "         ('his', 'lexicons'): 1,\n",
       "         ('his', 'and'): 22,\n",
       "         ('his', 'grammars'): 2,\n",
       "         ('his', ','): 45,\n",
       "         ('old', 'he'): 2,\n",
       "         ('old', 'was'): 1,\n",
       "         ('old', 'ever'): 1,\n",
       "         ('old', 'dusting'): 1,\n",
       "         ('old', 'his'): 3,\n",
       "         ('old', 'lexicons'): 1,\n",
       "         ('old', 'and'): 6,\n",
       "         ('old', 'grammars'): 2,\n",
       "         ('old', ','): 22,\n",
       "         ('old', 'with'): 5,\n",
       "         ('lexicons', 'was'): 1,\n",
       "         ('lexicons', 'ever'): 1,\n",
       "         ('lexicons', 'dusting'): 1,\n",
       "         ('lexicons', 'his'): 1,\n",
       "         ('lexicons', 'old'): 1,\n",
       "         ('lexicons', 'and'): 1,\n",
       "         ('lexicons', 'grammars'): 1,\n",
       "         ('lexicons', ','): 1,\n",
       "         ('lexicons', 'with'): 1,\n",
       "         ('lexicons', 'a'): 1,\n",
       "         ('and', 'ever'): 5,\n",
       "         ('and', 'dusting'): 1,\n",
       "         ('and', 'his'): 22,\n",
       "         ('and', 'old'): 6,\n",
       "         ('and', 'lexicons'): 1,\n",
       "         ('and', 'grammars'): 1,\n",
       "         ('and', 'with'): 13,\n",
       "         ('and', 'a'): 65,\n",
       "         ('and', 'queer'): 1,\n",
       "         ('grammars', 'dusting'): 1,\n",
       "         ('grammars', 'his'): 2,\n",
       "         ('grammars', 'old'): 2,\n",
       "         ('grammars', 'lexicons'): 1,\n",
       "         ('grammars', 'and'): 1,\n",
       "         ('grammars', ','): 1,\n",
       "         ('grammars', 'with'): 1,\n",
       "         ('grammars', 'a'): 1,\n",
       "         ('grammars', 'queer'): 1,\n",
       "         ('grammars', 'handkerchief'): 1,\n",
       "         (',', 'his'): 45,\n",
       "         (',', 'old'): 22,\n",
       "         (',', 'lexicons'): 1,\n",
       "         (',', 'grammars'): 1,\n",
       "         (',', 'with'): 50,\n",
       "         (',', 'a'): 167,\n",
       "         (',', 'queer'): 2,\n",
       "         (',', 'handkerchief'): 2,\n",
       "         ('with', 'old'): 5,\n",
       "         ('with', 'lexicons'): 1,\n",
       "         ('with', 'and'): 13,\n",
       "         ('with', 'grammars'): 1,\n",
       "         ('with', ','): 50,\n",
       "         ('with', 'a'): 22,\n",
       "         ('with', 'queer'): 2,\n",
       "         ('with', 'handkerchief'): 2,\n",
       "         ('with', 'mockingly'): 2,\n",
       "         ('a', 'lexicons'): 1,\n",
       "         ('a', 'and'): 65,\n",
       "         ('a', 'grammars'): 1,\n",
       "         ('a', ','): 167,\n",
       "         ('a', 'with'): 22,\n",
       "         ('a', 'queer'): 3,\n",
       "         ('a', 'handkerchief'): 1,\n",
       "         ('a', 'mockingly'): 1,\n",
       "         ('a', 'embellished'): 1,\n",
       "         ('queer', 'and'): 1,\n",
       "         ('queer', 'grammars'): 1,\n",
       "         ('queer', ','): 2,\n",
       "         ('queer', 'with'): 2,\n",
       "         ('queer', 'a'): 3,\n",
       "         ('queer', 'handkerchief'): 1,\n",
       "         ('queer', 'mockingly'): 1,\n",
       "         ('queer', 'embellished'): 1,\n",
       "         ('handkerchief', 'grammars'): 1,\n",
       "         ('handkerchief', ','): 2,\n",
       "         ('handkerchief', 'with'): 2,\n",
       "         ('handkerchief', 'a'): 1,\n",
       "         ('handkerchief', 'queer'): 1,\n",
       "         ('handkerchief', 'mockingly'): 1,\n",
       "         ('handkerchief', 'embellished'): 1,\n",
       "         ('handkerchief', 'all'): 1,\n",
       "         (',', 'mockingly'): 1,\n",
       "         (',', 'embellished'): 1,\n",
       "         (',', 'all'): 26,\n",
       "         (',', 'the'): 280,\n",
       "         ('mockingly', 'with'): 2,\n",
       "         ('mockingly', 'a'): 1,\n",
       "         ('mockingly', 'queer'): 1,\n",
       "         ('mockingly', 'handkerchief'): 1,\n",
       "         ('mockingly', ','): 1,\n",
       "         ('mockingly', 'embellished'): 1,\n",
       "         ('mockingly', 'all'): 1,\n",
       "         ('mockingly', 'the'): 1,\n",
       "         ('mockingly', 'gay'): 1,\n",
       "         ('embellished', 'a'): 1,\n",
       "         ('embellished', 'queer'): 1,\n",
       "         ('embellished', 'handkerchief'): 1,\n",
       "         ('embellished', ','): 1,\n",
       "         ('embellished', 'mockingly'): 1,\n",
       "         ('embellished', 'with'): 1,\n",
       "         ('embellished', 'all'): 1,\n",
       "         ('embellished', 'the'): 1,\n",
       "         ('embellished', 'gay'): 1,\n",
       "         ('embellished', 'flags'): 1,\n",
       "         ('with', 'embellished'): 1,\n",
       "         ('with', 'all'): 6,\n",
       "         ('with', 'the'): 30,\n",
       "         ('with', 'gay'): 1,\n",
       "         ('with', 'flags'): 1,\n",
       "         ('with', 'of'): 12,\n",
       "         ('all', 'handkerchief'): 1,\n",
       "         ('all', ','): 26,\n",
       "         ('all', 'mockingly'): 1,\n",
       "         ('all', 'embellished'): 1,\n",
       "         ('all', 'with'): 6,\n",
       "         ('all', 'the'): 27,\n",
       "         ('all', 'gay'): 2,\n",
       "         ('all', 'flags'): 2,\n",
       "         ('all', 'of'): 18,\n",
       "         ('all', 'all'): 2,\n",
       "         ('the', ','): 280,\n",
       "         ('the', 'mockingly'): 1,\n",
       "         ('the', 'embellished'): 1,\n",
       "         ('the', 'with'): 30,\n",
       "         ('the', 'all'): 27,\n",
       "         ('the', 'gay'): 2,\n",
       "         ('the', 'flags'): 2,\n",
       "         ('the', 'of'): 248,\n",
       "         ('the', 'the'): 220,\n",
       "         ('gay', 'mockingly'): 1,\n",
       "         ('gay', 'embellished'): 1,\n",
       "         ('gay', 'with'): 1,\n",
       "         ('gay', 'all'): 2,\n",
       "         ('gay', 'the'): 2,\n",
       "         ('gay', 'flags'): 1,\n",
       "         ('gay', 'of'): 1,\n",
       "         ('gay', 'known'): 1,\n",
       "         ('flags', 'embellished'): 1,\n",
       "         ('flags', 'with'): 1,\n",
       "         ('flags', 'all'): 2,\n",
       "         ('flags', 'the'): 2,\n",
       "         ('flags', 'gay'): 1,\n",
       "         ('flags', 'of'): 1,\n",
       "         ('flags', 'known'): 1,\n",
       "         ('flags', 'nations'): 1,\n",
       "         ('of', 'with'): 12,\n",
       "         ('of', 'all'): 18,\n",
       "         ('of', 'the'): 248,\n",
       "         ('of', 'gay'): 1,\n",
       "         ('of', 'flags'): 1,\n",
       "         ('of', 'known'): 5,\n",
       "         ('of', 'nations'): 3,\n",
       "         ('of', 'of'): 52,\n",
       "         ('all', 'known'): 2,\n",
       "         ('all', 'nations'): 1,\n",
       "         ('the', 'known'): 5,\n",
       "         ('the', 'nations'): 2,\n",
       "         ('the', 'world'): 11,\n",
       "         ('known', 'gay'): 1,\n",
       "         ('known', 'flags'): 1,\n",
       "         ('known', 'of'): 5,\n",
       "         ('known', 'all'): 2,\n",
       "         ('known', 'the'): 5,\n",
       "         ('known', 'nations'): 1,\n",
       "         ('known', 'world'): 1,\n",
       "         ('known', '.'): 1,\n",
       "         ('nations', 'flags'): 1,\n",
       "         ('nations', 'of'): 3,\n",
       "         ('nations', 'all'): 1,\n",
       "         ('nations', 'the'): 2,\n",
       "         ('nations', 'known'): 1,\n",
       "         ('nations', 'world'): 1,\n",
       "         ('nations', '.'): 1,\n",
       "         ('of', 'world'): 4,\n",
       "         ('of', '.'): 48,\n",
       "         ('the', '.'): 79,\n",
       "         ('world', 'the'): 11,\n",
       "         ('world', 'known'): 1,\n",
       "         ('world', 'nations'): 1,\n",
       "         ('world', 'of'): 4,\n",
       "         ('world', '.'): 4,\n",
       "         ('.', 'known'): 1,\n",
       "         ('.', 'nations'): 1,\n",
       "         ('.', 'of'): 48,\n",
       "         ('.', 'the'): 79,\n",
       "         ('.', 'world'): 4,\n",
       "         ('he', 'loved'): 1,\n",
       "         ('he', 'to'): 5,\n",
       "         ('he', 'dust'): 1,\n",
       "         ('loved', 'he'): 1,\n",
       "         ('loved', 'to'): 1,\n",
       "         ('loved', 'dust'): 1,\n",
       "         ('loved', 'his'): 1,\n",
       "         ('loved', 'old'): 1,\n",
       "         ('loved', 'grammars'): 1,\n",
       "         ('to', 'he'): 5,\n",
       "         ('to', 'loved'): 1,\n",
       "         ('to', 'dust'): 1,\n",
       "         ('to', 'his'): 4,\n",
       "         ('to', 'old'): 2,\n",
       "         ('to', 'grammars'): 1,\n",
       "         ('to', ';'): 13,\n",
       "         ('dust', 'he'): 1,\n",
       "         ('dust', 'loved'): 1,\n",
       "         ('dust', 'to'): 1,\n",
       "         ('dust', 'his'): 1,\n",
       "         ('dust', 'old'): 1,\n",
       "         ('dust', 'grammars'): 1,\n",
       "         ('dust', ';'): 1,\n",
       "         ('dust', 'it'): 1,\n",
       "         ('his', 'loved'): 1,\n",
       "         ('his', 'to'): 4,\n",
       "         ('his', 'dust'): 1,\n",
       "         ('his', ';'): 10,\n",
       "         ('his', 'it'): 2,\n",
       "         ('his', 'somehow'): 2,\n",
       "         ('old', 'loved'): 1,\n",
       "         ('old', 'to'): 2,\n",
       "         ('old', 'dust'): 1,\n",
       "         ('old', ';'): 1,\n",
       "         ('old', 'it'): 2,\n",
       "         ('old', 'somehow'): 1,\n",
       "         ('old', 'mildly'): 1,\n",
       "         ('grammars', 'loved'): 1,\n",
       "         ('grammars', 'to'): 1,\n",
       "         ('grammars', 'dust'): 1,\n",
       "         ('grammars', ';'): 1,\n",
       "         ('grammars', 'it'): 1,\n",
       "         ('grammars', 'somehow'): 1,\n",
       "         ('grammars', 'mildly'): 1,\n",
       "         ('grammars', 'reminded'): 1,\n",
       "         (';', 'to'): 13,\n",
       "         (';', 'dust'): 1,\n",
       "         (';', 'his'): 10,\n",
       "         (';', 'old'): 1,\n",
       "         (';', 'grammars'): 1,\n",
       "         (';', 'it'): 8,\n",
       "         (';', 'somehow'): 1,\n",
       "         (';', 'mildly'): 1,\n",
       "         (';', 'reminded'): 1,\n",
       "         ('it', 'dust'): 1,\n",
       "         ('it', 'his'): 2,\n",
       "         ('it', 'old'): 2,\n",
       "         ('it', 'grammars'): 1,\n",
       "         ('it', ';'): 8,\n",
       "         ('it', 'somehow'): 1,\n",
       "         ('it', 'mildly'): 1,\n",
       "         ('it', 'reminded'): 1,\n",
       "         ('it', 'him'): 1,\n",
       "         ('it', 'of'): 16,\n",
       "         ('somehow', 'his'): 2,\n",
       "         ('somehow', 'old'): 1,\n",
       "         ('somehow', 'grammars'): 1,\n",
       "         ('somehow', ';'): 1,\n",
       "         ('somehow', 'it'): 1,\n",
       "         ('somehow', 'mildly'): 1,\n",
       "         ('somehow', 'reminded'): 1,\n",
       "         ('somehow', 'him'): 1,\n",
       "         ('somehow', 'of'): 1,\n",
       "         ('mildly', 'old'): 1,\n",
       "         ('mildly', 'grammars'): 1,\n",
       "         ('mildly', ';'): 1,\n",
       "         ('mildly', 'it'): 1,\n",
       "         ('mildly', 'somehow'): 1,\n",
       "         ('mildly', 'reminded'): 1,\n",
       "         ('mildly', 'him'): 1,\n",
       "         ('mildly', 'of'): 1,\n",
       "         ('mildly', 'his'): 1,\n",
       "         ('mildly', 'mortality'): 1,\n",
       "         ('reminded', 'grammars'): 1,\n",
       "         ('reminded', ';'): 1,\n",
       "         ('reminded', 'it'): 1,\n",
       "         ('reminded', 'somehow'): 1,\n",
       "         ('reminded', 'mildly'): 1,\n",
       "         ('reminded', 'him'): 1,\n",
       "         ('reminded', 'of'): 1,\n",
       "         ('reminded', 'his'): 1,\n",
       "         ('reminded', 'mortality'): 1,\n",
       "         ('reminded', '.'): 1,\n",
       "         ('him', 'it'): 1,\n",
       "         ('him', 'somehow'): 1,\n",
       "         ('him', 'mildly'): 1,\n",
       "         ('him', 'reminded'): 1,\n",
       "         ('him', 'of'): 1,\n",
       "         ('him', 'his'): 1,\n",
       "         ('him', 'mortality'): 1,\n",
       "         ('of', 'it'): 16,\n",
       "         ('of', 'somehow'): 1,\n",
       "         ('of', 'mildly'): 1,\n",
       "         ('of', 'reminded'): 1,\n",
       "         ('of', 'him'): 1,\n",
       "         ('of', 'his'): 11,\n",
       "         ('of', 'mortality'): 1,\n",
       "         ('his', 'mildly'): 1,\n",
       "         ('his', 'reminded'): 1,\n",
       "         ('his', 'him'): 1,\n",
       "         ('his', 'of'): 11,\n",
       "         ('his', 'mortality'): 1,\n",
       "         ('his', '.'): 8,\n",
       "         ('mortality', 'mildly'): 1,\n",
       "         ('mortality', 'reminded'): 1,\n",
       "         ('mortality', 'him'): 1,\n",
       "         ('mortality', 'of'): 1,\n",
       "         ('mortality', 'his'): 1,\n",
       "         ('mortality', '.'): 1,\n",
       "         ('.', 'reminded'): 1,\n",
       "         ('.', 'his'): 8,\n",
       "         ('.', 'mortality'): 1,\n",
       "         ('\"', 'while'): 2,\n",
       "         ('\"', 'you'): 5,\n",
       "         ('\"', 'take'): 1,\n",
       "         ('\"', 'in'): 15,\n",
       "         ('\"', 'hand'): 1,\n",
       "         ('while', '\"'): 2,\n",
       "         ('while', 'you'): 1,\n",
       "         ('while', 'take'): 1,\n",
       "         ('while', 'in'): 1,\n",
       "         ('while', 'hand'): 1,\n",
       "         ('while', 'to'): 1,\n",
       "         ('you', '\"'): 5,\n",
       "         ('you', 'while'): 1,\n",
       "         ('you', 'take'): 3,\n",
       "         ('you', 'in'): 10,\n",
       "         ('you', 'hand'): 1,\n",
       "         ('you', 'to'): 16,\n",
       "         ('you', 'school'): 1,\n",
       "         ('take', '\"'): 1,\n",
       "         ('take', 'while'): 1,\n",
       "         ('take', 'you'): 3,\n",
       "         ('take', 'in'): 3,\n",
       "         ('take', 'hand'): 1,\n",
       "         ('take', 'to'): 5,\n",
       "         ('take', 'school'): 1,\n",
       "         ('take', 'others'): 1,\n",
       "         ('in', '\"'): 15,\n",
       "         ('in', 'while'): 1,\n",
       "         ('in', 'you'): 10,\n",
       "         ('in', 'take'): 3,\n",
       "         ('in', 'hand'): 1,\n",
       "         ('in', 'to'): 26,\n",
       "         ('in', 'school'): 1,\n",
       "         ('in', 'others'): 1,\n",
       "         ('hand', '\"'): 1,\n",
       "         ('hand', 'while'): 1,\n",
       "         ('hand', 'you'): 1,\n",
       "         ('hand', 'take'): 1,\n",
       "         ('hand', 'in'): 1,\n",
       "         ('hand', 'to'): 2,\n",
       "         ('hand', 'school'): 1,\n",
       "         ('hand', 'others'): 1,\n",
       "         ('hand', ','): 4,\n",
       "         ('hand', 'and'): 3,\n",
       "         ('to', 'while'): 1,\n",
       "         ('to', 'you'): 16,\n",
       "         ('to', 'take'): 5,\n",
       "         ('to', 'in'): 26,\n",
       "         ('to', 'hand'): 2,\n",
       "         ('to', 'others'): 2,\n",
       "         ('to', ','): 121,\n",
       "         ('to', 'and'): 36,\n",
       "         ('to', 'to'): 28,\n",
       "         ('school', 'you'): 1,\n",
       "         ('school', 'take'): 1,\n",
       "         ('school', 'in'): 1,\n",
       "         ('school', 'hand'): 1,\n",
       "         ('school', 'others'): 1,\n",
       "         ('school', ','): 1,\n",
       "         ('school', 'and'): 1,\n",
       "         ('school', 'teach'): 1,\n",
       "         ('others', 'take'): 1,\n",
       "         ('others', 'in'): 1,\n",
       "         ('others', 'hand'): 1,\n",
       "         ('others', 'to'): 2,\n",
       "         ('others', 'school'): 1,\n",
       "         ('others', ','): 2,\n",
       "         ('others', 'and'): 1,\n",
       "         ('others', 'teach'): 1,\n",
       "         ('others', 'them'): 1,\n",
       "         (',', 'hand'): 4,\n",
       "         (',', 'to'): 121,\n",
       "         (',', 'school'): 1,\n",
       "         (',', 'others'): 2,\n",
       "         (',', 'teach'): 1,\n",
       "         (',', 'them'): 9,\n",
       "         (',', 'by'): 32,\n",
       "         ('and', 'hand'): 3,\n",
       "         ('and', 'to'): 36,\n",
       "         ('and', 'school'): 1,\n",
       "         ('and', 'others'): 1,\n",
       "         ('and', 'teach'): 1,\n",
       "         ('and', 'them'): 4,\n",
       "         ('and', 'by'): 15,\n",
       "         ('and', 'what'): 8,\n",
       "         ('to', 'teach'): 1,\n",
       "         ('to', 'them'): 4,\n",
       "         ('to', 'what'): 4,\n",
       "         ('to', 'name'): 1,\n",
       "         ('teach', 'school'): 1,\n",
       "         ('teach', 'others'): 1,\n",
       "         ('teach', ','): 1,\n",
       "         ('teach', 'and'): 1,\n",
       "         ('teach', 'to'): 1,\n",
       "         ('teach', 'them'): 1,\n",
       "         ('teach', 'by'): 1,\n",
       "         ('teach', 'what'): 1,\n",
       "         ('teach', 'name'): 1,\n",
       "         ('teach', 'a'): 1,\n",
       "         ('them', 'others'): 1,\n",
       "         ('them', ','): 9,\n",
       "         ('them', 'and'): 4,\n",
       "         ('them', 'to'): 4,\n",
       "         ('them', 'teach'): 1,\n",
       "         ('them', 'by'): 2,\n",
       "         ('them', 'what'): 1,\n",
       "         ('them', 'name'): 1,\n",
       "         ('them', 'a'): 2,\n",
       "         ('them', 'whale'): 1,\n",
       "         ('by', ','): 32,\n",
       "         ('by', 'and'): 15,\n",
       "         ('by', 'teach'): 1,\n",
       "         ('by', 'them'): 2,\n",
       "         ('by', 'what'): 1,\n",
       "         ('by', 'name'): 2,\n",
       "         ('by', 'whale'): 9,\n",
       "         ('by', '-'): 6,\n",
       "         ('what', 'and'): 8,\n",
       "         ('what', 'to'): 4,\n",
       "         ('what', 'teach'): 1,\n",
       "         ('what', 'them'): 1,\n",
       "         ('what', 'by'): 1,\n",
       "         ('what', 'name'): 1,\n",
       "         ('what', 'a'): 5,\n",
       "         ('what', 'whale'): 1,\n",
       "         ('what', '-'): 2,\n",
       "         ('what', 'fish'): 1,\n",
       "         ('name', 'to'): 1,\n",
       "         ('name', 'teach'): 1,\n",
       "         ('name', 'them'): 1,\n",
       "         ('name', 'by'): 2,\n",
       "         ('name', 'what'): 1,\n",
       "         ('name', 'a'): 2,\n",
       "         ('name', 'whale'): 1,\n",
       "         ('name', '-'): 1,\n",
       "         ('name', 'fish'): 1,\n",
       "         ('name', 'is'): 2,\n",
       "         ('a', 'teach'): 1,\n",
       "         ('a', 'them'): 2,\n",
       "         ('a', 'what'): 5,\n",
       "         ('a', 'name'): 2,\n",
       "         ('a', 'whale'): 26,\n",
       "         ('a', '-'): 28,\n",
       "         ('a', 'fish'): 4,\n",
       "         ('a', 'is'): 21,\n",
       "         ('whale', 'them'): 1,\n",
       "         ('whale', 'by'): 9,\n",
       "         ('whale', 'what'): 1,\n",
       "         ('whale', 'name'): 1,\n",
       "         ('whale', 'a'): 26,\n",
       "         ('whale', '-'): 13,\n",
       "         ('whale', 'fish'): 2,\n",
       "         ('whale', 'is'): 10,\n",
       "         ('whale', 'to'): 15,\n",
       "         ('whale', 'be'): 4,\n",
       "         ('-', 'by'): 6,\n",
       "         ('-', 'what'): 2,\n",
       "         ('-', 'name'): 1,\n",
       "         ('-', 'a'): 28,\n",
       "         ('-', 'whale'): 13,\n",
       "         ('-', 'fish'): 3,\n",
       "         ('-', 'is'): 3,\n",
       "         ('-', 'to'): 13,\n",
       "         ('-', 'be'): 4,\n",
       "         ('-', 'called'): 3,\n",
       "         ('fish', 'what'): 1,\n",
       "         ('fish', 'name'): 1,\n",
       "         ('fish', 'a'): 4,\n",
       "         ('fish', 'whale'): 2,\n",
       "         ('fish', '-'): 3,\n",
       "         ('fish', 'is'): 3,\n",
       "         ('fish', 'to'): 4,\n",
       "         ('fish', 'be'): 1,\n",
       "         ('fish', 'called'): 1,\n",
       "         ('fish', 'in'): 1,\n",
       "         ('is', 'name'): 2,\n",
       "         ('is', 'a'): 21,\n",
       "         ('is', 'whale'): 10,\n",
       "         ('is', '-'): 3,\n",
       "         ('is', 'fish'): 3,\n",
       "         ('is', 'to'): 11,\n",
       "         ('is', 'be'): 3,\n",
       "         ('is', 'called'): 1,\n",
       "         ('is', 'in'): 14,\n",
       "         ('is', 'our'): 2,\n",
       "         ('to', 'whale'): 15,\n",
       "         ('to', '-'): 13,\n",
       "         ('to', 'fish'): 4,\n",
       "         ('to', 'is'): 11,\n",
       "         ('to', 'be'): 13,\n",
       "         ('to', 'called'): 1,\n",
       "         ('to', 'our'): 1,\n",
       "         ('to', 'tongue'): 1,\n",
       "         ('be', 'whale'): 4,\n",
       "         ('be', '-'): 4,\n",
       "         ('be', 'fish'): 1,\n",
       "         ('be', 'is'): 3,\n",
       "         ('be', 'to'): 13,\n",
       "         ('be', 'called'): 1,\n",
       "         ('be', 'in'): 8,\n",
       "         ('be', 'our'): 1,\n",
       "         ('be', 'tongue'): 1,\n",
       "         ('be', 'leaving'): 1,\n",
       "         ('called', '-'): 3,\n",
       "         ('called', 'fish'): 1,\n",
       "         ('called', 'is'): 1,\n",
       "         ('called', 'to'): 1,\n",
       "         ('called', 'be'): 1,\n",
       "         ('called', 'in'): 3,\n",
       "         ('called', 'our'): 1,\n",
       "         ('called', 'tongue'): 1,\n",
       "         ('called', 'leaving'): 1,\n",
       "         ('called', 'out'): 1,\n",
       "         ('in', 'fish'): 1,\n",
       "         ('in', 'is'): 14,\n",
       "         ('in', 'be'): 8,\n",
       "         ('in', 'called'): 3,\n",
       "         ('in', 'our'): 1,\n",
       "         ('in', 'tongue'): 1,\n",
       "         ('in', 'leaving'): 1,\n",
       "         ('in', 'out'): 3,\n",
       "         ('our', 'is'): 2,\n",
       "         ('our', 'to'): 1,\n",
       "         ('our', 'be'): 1,\n",
       "         ('our', 'called'): 1,\n",
       "         ('our', 'in'): 1,\n",
       "         ('our', 'tongue'): 1,\n",
       "         ('our', 'leaving'): 1,\n",
       "         ('our', 'out'): 1,\n",
       "         ('our', ','): 3,\n",
       "         ('our', 'through'): 1,\n",
       "         ('tongue', 'to'): 1,\n",
       "         ('tongue', 'be'): 1,\n",
       "         ('tongue', 'called'): 1,\n",
       "         ('tongue', 'in'): 1,\n",
       "         ('tongue', 'our'): 1,\n",
       "         ('tongue', 'leaving'): 1,\n",
       "         ('tongue', 'out'): 1,\n",
       "         ('tongue', ','): 1,\n",
       "         ('tongue', 'through'): 1,\n",
       "         ('tongue', 'ignorance'): 1,\n",
       "         ('leaving', 'be'): 1,\n",
       "         ('leaving', 'called'): 1,\n",
       "         ('leaving', 'in'): 1,\n",
       "         ('leaving', 'our'): 1,\n",
       "         ('leaving', 'tongue'): 1,\n",
       "         ('leaving', 'out'): 1,\n",
       "         ('leaving', ','): 2,\n",
       "         ('leaving', 'through'): 1,\n",
       "         ('leaving', 'ignorance'): 1,\n",
       "         ('out', 'called'): 1,\n",
       "         ('out', 'in'): 3,\n",
       "         ('out', 'our'): 1,\n",
       "         ('out', 'tongue'): 1,\n",
       "         ('out', 'leaving'): 1,\n",
       "         ('out', ','): 10,\n",
       "         ('out', 'through'): 1,\n",
       "         ('out', 'ignorance'): 1,\n",
       "         ('out', 'the'): 10,\n",
       "         (',', 'our'): 3,\n",
       "         (',', 'tongue'): 1,\n",
       "         (',', 'leaving'): 2,\n",
       "         (',', 'out'): 10,\n",
       "         (',', 'through'): 8,\n",
       "         (',', 'ignorance'): 3,\n",
       "         (',', 'letter'): 4,\n",
       "         ('through', 'our'): 1,\n",
       "         ('through', 'tongue'): 1,\n",
       "         ('through', 'leaving'): 1,\n",
       "         ('through', 'out'): 1,\n",
       "         ('through', ','): 8,\n",
       "         ('through', 'ignorance'): 1,\n",
       "         ('through', 'the'): 6,\n",
       "         ('through', 'letter'): 1,\n",
       "         ('through', 'h'): 1,\n",
       "         ('ignorance', 'tongue'): 1,\n",
       "         ('ignorance', 'leaving'): 1,\n",
       "         ('ignorance', 'out'): 1,\n",
       "         ('ignorance', ','): 3,\n",
       "         ('ignorance', 'through'): 1,\n",
       "         ('ignorance', 'the'): 1,\n",
       "         ('ignorance', 'letter'): 1,\n",
       "         ('ignorance', 'h'): 1,\n",
       "         (',', 'h'): 2,\n",
       "         (',', 'which'): 31,\n",
       "         ('the', 'out'): 10,\n",
       "         ('the', 'through'): 6,\n",
       "         ('the', 'ignorance'): 1,\n",
       "         ('the', 'letter'): 2,\n",
       "         ('the', 'h'): 1,\n",
       "         ('the', 'which'): 18,\n",
       "         ('the', 'almost'): 3,\n",
       "         ('letter', ','): 4,\n",
       "         ('letter', 'through'): 1,\n",
       "         ('letter', 'ignorance'): 1,\n",
       "         ('letter', 'the'): 2,\n",
       "         ('letter', 'h'): 1,\n",
       "         ('letter', 'which'): 1,\n",
       "         ('letter', 'almost'): 1,\n",
       "         ('letter', 'alone'): 1,\n",
       "         ('h', 'through'): 1,\n",
       "         ('h', 'ignorance'): 1,\n",
       "         ('h', ','): 2,\n",
       "         ('h', 'the'): 1,\n",
       "         ('h', 'letter'): 1,\n",
       "         ('h', 'which'): 1,\n",
       "         ('h', 'almost'): 1,\n",
       "         ('h', 'alone'): 1,\n",
       "         ('h', 'maketh'): 1,\n",
       "         (',', 'almost'): 8,\n",
       "         (',', 'alone'): 1,\n",
       "         (',', 'maketh'): 2,\n",
       "         ('which', ','): 31,\n",
       "         ('which', 'the'): 18,\n",
       "         ('which', 'letter'): 1,\n",
       "         ('which', 'h'): 1,\n",
       "         ('which', 'almost'): 1,\n",
       "         ('which', 'alone'): 1,\n",
       "         ('which', 'maketh'): 1,\n",
       "         ('which', 'signification'): 1,\n",
       "         ('almost', 'the'): 3,\n",
       "         ('almost', 'letter'): 1,\n",
       "         ('almost', 'h'): 1,\n",
       "         ('almost', ','): 8,\n",
       "         ('almost', 'which'): 1,\n",
       "         ('almost', 'alone'): 1,\n",
       "         ('almost', 'maketh'): 1,\n",
       "         ('almost', 'signification'): 1,\n",
       "         ('almost', 'of'): 1,\n",
       "         ('alone', 'letter'): 1,\n",
       "         ('alone', 'h'): 1,\n",
       "         ('alone', ','): 1,\n",
       "         ('alone', 'which'): 1,\n",
       "         ('alone', 'almost'): 1,\n",
       "         ('alone', 'maketh'): 1,\n",
       "         ('alone', 'the'): 2,\n",
       "         ('alone', 'signification'): 1,\n",
       "         ('alone', 'of'): 1,\n",
       "         ('maketh', 'h'): 1,\n",
       "         ('maketh', ','): 2,\n",
       "         ('maketh', 'which'): 1,\n",
       "         ('maketh', 'almost'): 1,\n",
       "         ('maketh', 'alone'): 1,\n",
       "         ('maketh', 'the'): 4,\n",
       "         ('maketh', 'signification'): 1,\n",
       "         ('maketh', 'of'): 1,\n",
       "         ('maketh', 'word'): 1,\n",
       "         ('the', 'alone'): 2,\n",
       "         ('the', 'maketh'): 4,\n",
       "         ('the', 'signification'): 2,\n",
       "         ('the', 'word'): 2,\n",
       "         ('signification', 'which'): 1,\n",
       "         ('signification', 'almost'): 1,\n",
       "         ('signification', 'alone'): 1,\n",
       "         ('signification', 'maketh'): 1,\n",
       "         ('signification', 'the'): 2,\n",
       "         ('signification', 'of'): 1,\n",
       "         ('signification', 'word'): 1,\n",
       "         ('signification', ','): 1,\n",
       "         ('signification', 'you'): 1,\n",
       "         ('of', 'almost'): 1,\n",
       "         ('of', 'alone'): 1,\n",
       "         ('of', 'maketh'): 1,\n",
       "         ('of', 'signification'): 1,\n",
       "         ('of', 'word'): 1,\n",
       "         ('of', ','): 164,\n",
       "         ('of', 'you'): 9,\n",
       "         ('of', 'deliver'): 1,\n",
       "         ('the', 'you'): 16,\n",
       "         ('the', 'deliver'): 1,\n",
       "         ('the', 'that'): 47,\n",
       "         ('word', 'maketh'): 1,\n",
       "         ('word', 'the'): 2,\n",
       "         ('word', 'signification'): 1,\n",
       "         ('word', 'of'): 1,\n",
       "         ('word', ','): 1,\n",
       "         ('word', 'you'): 1,\n",
       "         ('word', 'deliver'): 1,\n",
       "         ('word', 'that'): 1,\n",
       "         ('word', 'which'): 1,\n",
       "         (',', 'signification'): 1,\n",
       "         (',', 'of'): 164,\n",
       "         (',', 'word'): 1,\n",
       "         ...})"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ik_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ik = {}\n",
    "weighting_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the possible subsets or arrangements of the iterator\n",
    "# Elements are allowed to repeat in a subset because of the word 'replacement'\n",
    "# In fact, combinations_with_replacement(vocab, 2) is a matrix with size len(vocab) x len(vocab)\n",
    "# The following line list the first five samples  \n",
    "vocab_mat = list(combinations_with_replacement(vocab, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('return', 'return'), ('return', 'elements'), ('return', 'exercise'), ('return', 'savages'), ('return', 'holding')]\n"
     ]
    }
   ],
   "source": [
    "print(vocab_mat[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the weighting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighting(w_i, w_j):\n",
    "    try:\n",
    "        x_ij = X_ik[(w_i, w_j)]\n",
    "    except:\n",
    "        x_ij = 1\n",
    "        \n",
    "    x_max = 100 #100 # fixed in paper\n",
    "    alpha = 0.75\n",
    "    \n",
    "    if x_ij < x_max:\n",
    "        result = (x_ij/x_max)**alpha\n",
    "    else:\n",
    "        result = 1\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3399528/3399528 [00:11<00:00, 283462.07it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(vocab_mat))):\n",
    "    pair = vocab_mat[i]\n",
    "    if X_ik_window.get(pair) is not None: # If the pair exists in text (sentence)\n",
    "        co_occur = X_ik_window[pair]      # Get its co-occurence time\n",
    "        X_ik[(pair[0], pair[1])] = co_occur + 1 # log(Xik) -> log(Xik+1) to prevent divergence\n",
    "        X_ik[(pair[1], pair[0])] = co_occur + 1 # Symmetry \n",
    "    else: # If the pair not exist, then do nothing\n",
    "        pass\n",
    "\n",
    "    # Compute the weight of the pair, symmetrically\n",
    "    weighting_dict[(pair[0], pair[1])] = weighting(pair[0], pair[1])\n",
    "    weighting_dict[(pair[1], pair[0])] = weighting(pair[1], pair[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = []\n",
    "context = []\n",
    "co_occur = []\n",
    "weights = []\n",
    "\n",
    "for pair in word_pairs:\n",
    "    # Convert the word into a tensor after integer representation\n",
    "    word.append(torch.tensor(word2index[pair[0]], device='cuda').view(1, -1))\n",
    "    context.append(torch.tensor(word2index[pair[1]], device='cuda').view(1, -1))\n",
    "\n",
    "    try:\n",
    "        co_occur_count = torch.tensor([X_ik[pair]], device='cuda').view(1, -1)\n",
    "    except:\n",
    "        co_occur_count = torch.tensor([1.0], device='cuda').view(1, -1)\n",
    "    \n",
    "    # Record the log of the co-occurence and the weight of the pair\n",
    "    co_occur.append(torch.log(co_occur_count))\n",
    "    weights.append(torch.tensor([weighting_dict[pair]], device='cuda').view(1, -1))\n",
    "\n",
    "# Gather all four list together as the training data\n",
    "train_data = list(zip(word, context, co_occur, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('[', 'moby')\n",
      "(tensor([[2515]], device='cuda:0'), tensor([[1006]], device='cuda:0'), tensor([[0.6931]], device='cuda:0'), tensor([[0.0532]], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "print(word_pairs[0])\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloVe(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, projection_dim):\n",
    "        super(GloVe, self).__init__()\n",
    "        self.embedding_v = nn.Embedding(vocab_size, projection_dim) # center embedding\n",
    "        self.embedding_u = nn.Embedding(vocab_size, projection_dim) # out embedding\n",
    "        \n",
    "        self.v_bias = nn.Embedding(vocab_size, 1)\n",
    "        self.u_bias = nn.Embedding(vocab_size, 1)\n",
    "        \n",
    "        initrange = (2.0 / (vocab_size + projection_dim))**0.5 # Xavier init\n",
    "        self.embedding_v.weight.data.uniform_(-initrange, initrange) # init\n",
    "        self.embedding_u.weight.data.uniform_(-initrange, initrange) # init\n",
    "        self.v_bias.weight.data.uniform_(-initrange, initrange) # init\n",
    "        self.u_bias.weight.data.uniform_(-initrange, initrange) # init\n",
    "        \n",
    "    def forward(self, center_words, target_words, coocs, weights):\n",
    "        center_embeds = self.embedding_v(center_words) # B x 1 x D\n",
    "        target_embeds = self.embedding_u(target_words) # B x 1 x D\n",
    "        \n",
    "        center_bias = self.v_bias(center_words).squeeze(1)\n",
    "        target_bias = self.u_bias(target_words).squeeze(1)\n",
    "        \n",
    "        inner_product = target_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2) # Bx1\n",
    "        \n",
    "        loss = weights * torch.pow(inner_product + center_bias + target_bias - coocs, 2)\n",
    "        \n",
    "        return torch.sum(loss)\n",
    "    \n",
    "    def prediction(self, inputs):\n",
    "        v_embeds = self.embedding_v(inputs) # B x 1 x D\n",
    "        u_embeds = self.embedding_u(inputs) # B x 1 x D\n",
    "                \n",
    "        return v_embeds + u_embeds # final embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 50\n",
    "BATCH_SIZE = 256\n",
    "EPOCH = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "model = GloVe(len(word2index), EMBEDDING_SIZE)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch_size, train_data):\n",
    "    random.shuffle(train_data) # Shuffle all samples in the dataset\n",
    "    start_index = 0     \n",
    "    end_index = batch_size\n",
    "\n",
    "    while end_index < len(train_data):\n",
    "        # Slide samples from start_index to end_index\n",
    "        batch = train_data[start_index:end_index]\n",
    "        # Update start and end index to point to the next batch\n",
    "        temp = end_index\n",
    "        end_index += batch_size\n",
    "        start_index = temp\n",
    "        yield batch # MUse yield to remember the index in the next calling\n",
    "    \n",
    "    if end_index >= len(train_data):\n",
    "        batch = train_data[start_index:] # Get all rest of samples to be the batch\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0, mean_loss : 215.86\n",
      "Epoch : 10, mean_loss : 2.37\n",
      "Epoch : 20, mean_loss : 0.52\n",
      "Epoch : 30, mean_loss : 0.12\n",
      "Epoch : 40, mean_loss : 0.04\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCH):\n",
    "    for i, batch in enumerate(get_batch(BATCH_SIZE, train_data)):\n",
    "        \n",
    "        inputs, targets, coocs, weights = zip(*batch)\n",
    "        \n",
    "        inputs = torch.cat(inputs) # B x 1\n",
    "        targets = torch.cat(targets) # B x 1\n",
    "        coocs = torch.cat(coocs)\n",
    "        weights = torch.cat(weights)\n",
    "        model.zero_grad()\n",
    "\n",
    "        loss = model(inputs, targets, coocs, weights)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        losses.append(loss.data.tolist())\n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Epoch : %d, mean_loss : %.02f\" % (epoch, np.mean(losses)))\n",
    "        losses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_similarity(target, vocab, n_words):\n",
    "    # Encode the target words\n",
    "    target_V = model.prediction(torch.tensor(word2index[target], device='cuda'))\n",
    "\n",
    "    similarities = []\n",
    "    for i in range(len(vocab)):\n",
    "        if vocab[i] == target: # Skip the word itself\n",
    "            continue\n",
    "        \n",
    "        # Use other words for comparison and distance computation using cosine similarity\n",
    "        vector = model.prediction(torch.tensor(word2index[vocab[i]], device='cuda'))\n",
    "        cosine_sim = F.cosine_similarity(target_V, vector, 0).data.tolist()\n",
    "\n",
    "        similarities.append([vocab[i], cosine_sim])\n",
    "    \n",
    "    # Return the most similar n_words according to the cosine similarity\n",
    "    return sorted(similarities, key=lambda x: x[1], reverse=True)[:n_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random choose: attempt\n",
      "10 most related words:\n",
      "['rude', 0.8951979279518127]\n",
      "['sprat', 0.8603125810623169]\n",
      "['demanded', 0.8229526281356812]\n",
      "['barques', 0.8207601308822632]\n",
      "['captains', 0.816744327545166]\n",
      "['beast', 0.8139459490776062]\n",
      "['pulpit', 0.8119369745254517]\n",
      "['schoolmaster', 0.8063094615936279]\n",
      "['gateway', 0.8044277429580688]\n",
      "['very', 0.7918776869773865]\n"
     ]
    }
   ],
   "source": [
    "test = random.choice(vocab)\n",
    "print(f'Random choose: {test}')\n",
    "print(f'10 most related words:')\n",
    "for pred in word_similarity(test, vocab, 10):\n",
    "    print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e76b06f811d914f25ddf1d876c9e6424e54248baadb52cf54ff8d72e027625bb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
